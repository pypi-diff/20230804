# Comparing `tmp/alipai-0.4.1-py2.py3-none-any.whl.zip` & `tmp/alipai-0.4.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,84 +1,85 @@
-Zip file size: 252149 bytes, number of entries: 82
--rw-r--r--  2.0 unx        6 b- defN 23-Jun-30 07:11 pai/VERSION
--rw-r--r--  2.0 unx      290 b- defN 23-May-18 09:03 pai/__init__.py
--rw-r--r--  2.0 unx    40574 b- defN 23-Jun-30 06:33 pai/estimator.py
--rw-r--r--  2.0 unx      489 b- defN 23-May-24 02:33 pai/exception.py
--rw-r--r--  2.0 unx    12163 b- defN 23-Jun-20 07:36 pai/image.py
--rw-r--r--  2.0 unx    40522 b- defN 23-Jun-16 09:21 pai/model.py
--rw-r--r--  2.0 unx    46831 b- defN 23-Jun-16 09:21 pai/predictor.py
--rw-r--r--  2.0 unx    20082 b- defN 23-May-19 07:10 pai/serializers.py
--rw-r--r--  2.0 unx    14538 b- defN 23-Jun-27 08:28 pai/session.py
--rw-r--r--  2.0 unx       39 b- defN 22-Oct-31 10:14 pai/api/__init__.py
--rw-r--r--  2.0 unx     4249 b- defN 23-May-19 07:10 pai/api/algorithm.py
--rw-r--r--  2.0 unx     7935 b- defN 23-Jun-30 03:18 pai/api/api_container.py
--rw-r--r--  2.0 unx     5439 b- defN 23-Jun-29 05:41 pai/api/base.py
--rw-r--r--  2.0 unx     4010 b- defN 23-Jun-15 12:22 pai/api/client_factory.py
--rw-r--r--  2.0 unx     2945 b- defN 23-May-19 07:10 pai/api/code_source.py
--rw-r--r--  2.0 unx     3755 b- defN 23-May-19 07:10 pai/api/dataset.py
--rw-r--r--  2.0 unx     1726 b- defN 23-May-19 07:10 pai/api/entity_base.py
--rw-r--r--  2.0 unx     2665 b- defN 23-May-31 10:09 pai/api/image.py
--rw-r--r--  2.0 unx     5232 b- defN 23-May-19 07:10 pai/api/job.py
--rw-r--r--  2.0 unx     6090 b- defN 23-May-19 07:10 pai/api/model.py
--rw-r--r--  2.0 unx     3426 b- defN 23-May-19 07:10 pai/api/pipeline.py
--rw-r--r--  2.0 unx     5314 b- defN 23-May-19 07:10 pai/api/pipeline_run.py
--rw-r--r--  2.0 unx     5421 b- defN 23-Jun-30 03:18 pai/api/service.py
--rw-r--r--  2.0 unx     5410 b- defN 23-May-19 07:10 pai/api/training_job.py
--rw-r--r--  2.0 unx     8340 b- defN 23-May-19 07:10 pai/api/workspace.py
--rw-r--r--  2.0 unx       67 b- defN 22-Oct-31 10:14 pai/common/__init__.py
--rw-r--r--  2.0 unx     1565 b- defN 23-May-31 10:09 pai/common/consts.py
--rw-r--r--  2.0 unx     6438 b- defN 23-May-31 10:09 pai/common/docker_utils.py
--rw-r--r--  2.0 unx    12168 b- defN 23-Jun-15 12:27 pai/common/git_utils.py
--rw-r--r--  2.0 unx    14998 b- defN 23-May-19 10:13 pai/common/oss_utils.py
--rw-r--r--  2.0 unx     3406 b- defN 23-Jun-05 08:54 pai/common/utils.py
--rw-r--r--  2.0 unx      710 b- defN 23-May-18 09:03 pai/common/yaml_utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-19 10:13 pai/huggingface/__init__.py
--rw-r--r--  2.0 unx    10037 b- defN 23-Jun-15 12:27 pai/huggingface/estimator.py
--rw-r--r--  2.0 unx    12791 b- defN 23-Jun-15 12:27 pai/huggingface/model.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 11:52 pai/libs/__init__.py
--rw-r--r--  2.0 unx       22 b- defN 23-May-31 10:09 pai/libs/alibabacloud_aiworkspace20210204/__init__.py
--rw-r--r--  2.0 unx   278209 b- defN 23-May-31 10:09 pai/libs/alibabacloud_aiworkspace20210204/client.py
--rw-r--r--  2.0 unx   400899 b- defN 23-May-31 10:09 pai/libs/alibabacloud_aiworkspace20210204/models.py
--rw-r--r--  2.0 unx       21 b- defN 23-May-25 08:27 pai/libs/alibabacloud_eas20210701/__init__.py
--rw-r--r--  2.0 unx   188668 b- defN 23-May-25 08:27 pai/libs/alibabacloud_eas20210701/client.py
--rw-r--r--  2.0 unx   251200 b- defN 23-May-25 08:27 pai/libs/alibabacloud_eas20210701/models.py
--rw-r--r--  2.0 unx       21 b- defN 23-May-18 09:03 pai/libs/alibabacloud_pai_dlc20201203/__init__.py
--rw-r--r--  2.0 unx    73033 b- defN 23-May-25 02:46 pai/libs/alibabacloud_pai_dlc20201203/client.py
--rw-r--r--  2.0 unx   176083 b- defN 23-May-18 09:03 pai/libs/alibabacloud_pai_dlc20201203/models.py
--rw-r--r--  2.0 unx       21 b- defN 23-Mar-29 11:52 pai/libs/alibabacloud_paiflow20210202/__init__.py
--rw-r--r--  2.0 unx    76356 b- defN 23-May-18 09:03 pai/libs/alibabacloud_paiflow20210202/client.py
--rw-r--r--  2.0 unx   116997 b- defN 23-May-18 09:03 pai/libs/alibabacloud_paiflow20210202/models.py
--rw-r--r--  2.0 unx       22 b- defN 23-Jun-20 06:21 pai/libs/alibabacloud_paistudio20220112/__init__.py
--rw-r--r--  2.0 unx   107153 b- defN 23-Jun-29 05:53 pai/libs/alibabacloud_paistudio20220112/client.py
--rw-r--r--  2.0 unx   236081 b- defN 23-Jun-30 03:18 pai/libs/alibabacloud_paistudio20220112/models.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-19 10:13 pai/modelscope/__init__.py
--rw-r--r--  2.0 unx     9985 b- defN 23-Jun-15 12:27 pai/modelscope/estimator.py
--rw-r--r--  2.0 unx    12704 b- defN 23-Jun-15 12:27 pai/modelscope/model.py
--rw-r--r--  2.0 unx      503 b- defN 23-May-19 07:10 pai/pipeline/__init__.py
--rw-r--r--  2.0 unx     2463 b- defN 23-May-18 09:03 pai/pipeline/artifact.py
--rw-r--r--  2.0 unx       81 b- defN 22-Oct-31 10:14 pai/pipeline/consts.py
--rw-r--r--  2.0 unx    13115 b- defN 23-May-18 09:03 pai/pipeline/core.py
--rw-r--r--  2.0 unx    15710 b- defN 23-May-19 07:10 pai/pipeline/run.py
--rw-r--r--  2.0 unx    12848 b- defN 23-May-18 09:03 pai/pipeline/step.py
--rw-r--r--  2.0 unx      128 b- defN 23-May-19 07:10 pai/pipeline/component/__init__.py
--rw-r--r--  2.0 unx     9801 b- defN 23-May-18 09:03 pai/pipeline/component/_base.py
--rw-r--r--  2.0 unx     2681 b- defN 23-May-18 09:03 pai/pipeline/component/_container.py
--rw-r--r--  2.0 unx     8522 b- defN 23-May-22 11:02 pai/pipeline/component/_registered.py
--rw-r--r--  2.0 unx      667 b- defN 23-May-18 09:03 pai/pipeline/types/__init__.py
--rw-r--r--  2.0 unx    25584 b- defN 23-May-19 07:10 pai/pipeline/types/artifact.py
--rw-r--r--  2.0 unx     9725 b- defN 23-May-18 09:03 pai/pipeline/types/parameter.py
--rw-r--r--  2.0 unx     6055 b- defN 23-May-18 09:03 pai/pipeline/types/spec.py
--rw-r--r--  2.0 unx     4249 b- defN 23-May-18 09:03 pai/pipeline/types/variable.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-19 07:10 pai/schema/__init__.py
--rw-r--r--  2.0 unx     2871 b- defN 23-May-18 09:03 pai/schema/base.py
--rw-r--r--  2.0 unx     2857 b- defN 23-May-18 09:03 pai/schema/training_job_schema.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-19 07:10 pai/toolkit/__init__.py
--rw-r--r--  2.0 unx    16185 b- defN 23-May-19 07:10 pai/toolkit/config.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-19 07:10 pai/toolkit/helper/__init__.py
--rw-r--r--  2.0 unx     2559 b- defN 23-May-19 07:10 pai/toolkit/helper/consts.py
--rw-r--r--  2.0 unx    10835 b- defN 23-May-19 07:10 pai/toolkit/helper/utils.py
--rw-r--r--  2.0 unx    11358 b- defN 23-Jun-30 07:12 alipai-0.4.1.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     1250 b- defN 23-Jun-30 07:12 alipai-0.4.1.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Jun-30 07:12 alipai-0.4.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Jun-30 07:12 alipai-0.4.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6880 b- defN 23-Jun-30 07:12 alipai-0.4.1.dist-info/RECORD
-82 files, 2404187 bytes uncompressed, 241353 bytes compressed:  90.0%
+Zip file size: 262679 bytes, number of entries: 83
+-rw-r--r--  2.0 unx      368 b- defN 23-Jul-13 10:51 pai/__init__.py
+-rw-r--r--  2.0 unx    70483 b- defN 23-Aug-04 06:25 pai/estimator.py
+-rw-r--r--  2.0 unx      729 b- defN 23-Jul-27 09:37 pai/exception.py
+-rw-r--r--  2.0 unx    12163 b- defN 23-Jun-26 07:00 pai/image.py
+-rw-r--r--  2.0 unx    64524 b- defN 23-Aug-04 06:25 pai/model.py
+-rw-r--r--  2.0 unx    46831 b- defN 23-Jun-26 07:00 pai/predictor.py
+-rw-r--r--  2.0 unx    20380 b- defN 23-Aug-04 06:25 pai/serializers.py
+-rw-r--r--  2.0 unx    14538 b- defN 23-Aug-03 08:07 pai/session.py
+-rw-r--r--  2.0 unx     6141 b- defN 23-Jul-27 09:37 pai/tensorboard.py
+-rw-r--r--  2.0 unx       39 b- defN 23-Mar-01 09:35 pai/api/__init__.py
+-rw-r--r--  2.0 unx     4249 b- defN 23-Jul-03 09:40 pai/api/algorithm.py
+-rw-r--r--  2.0 unx     8360 b- defN 23-Jul-27 09:37 pai/api/api_container.py
+-rw-r--r--  2.0 unx     5813 b- defN 23-Jul-27 09:37 pai/api/base.py
+-rw-r--r--  2.0 unx     4010 b- defN 23-Jun-26 07:00 pai/api/client_factory.py
+-rw-r--r--  2.0 unx     2945 b- defN 23-Jun-26 07:00 pai/api/code_source.py
+-rw-r--r--  2.0 unx     3755 b- defN 23-Jun-26 07:00 pai/api/dataset.py
+-rw-r--r--  2.0 unx     1726 b- defN 23-Jun-26 07:00 pai/api/entity_base.py
+-rw-r--r--  2.0 unx     2665 b- defN 23-Jun-26 07:00 pai/api/image.py
+-rw-r--r--  2.0 unx     5232 b- defN 23-Jun-26 07:00 pai/api/job.py
+-rw-r--r--  2.0 unx     7497 b- defN 23-Aug-03 08:07 pai/api/model.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-Jun-26 07:00 pai/api/pipeline.py
+-rw-r--r--  2.0 unx     5314 b- defN 23-Jun-26 07:00 pai/api/pipeline_run.py
+-rw-r--r--  2.0 unx     5421 b- defN 23-Jun-26 07:00 pai/api/service.py
+-rw-r--r--  2.0 unx     3374 b- defN 23-Aug-03 08:07 pai/api/tensorboard.py
+-rw-r--r--  2.0 unx     5466 b- defN 23-Jul-27 09:37 pai/api/training_job.py
+-rw-r--r--  2.0 unx     8340 b- defN 23-Jun-26 07:00 pai/api/workspace.py
+-rw-r--r--  2.0 unx       67 b- defN 23-Mar-01 09:35 pai/common/__init__.py
+-rw-r--r--  2.0 unx     1565 b- defN 23-Jun-26 07:00 pai/common/consts.py
+-rw-r--r--  2.0 unx     6629 b- defN 23-Jul-27 09:37 pai/common/docker_utils.py
+-rw-r--r--  2.0 unx    12168 b- defN 23-Jun-26 07:00 pai/common/git_utils.py
+-rw-r--r--  2.0 unx    14998 b- defN 23-Jun-26 07:00 pai/common/oss_utils.py
+-rw-r--r--  2.0 unx     3121 b- defN 23-Jul-27 09:37 pai/common/utils.py
+-rw-r--r--  2.0 unx      710 b- defN 23-Mar-01 09:35 pai/common/yaml_utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:00 pai/huggingface/__init__.py
+-rw-r--r--  2.0 unx    10037 b- defN 23-Jun-26 07:00 pai/huggingface/estimator.py
+-rw-r--r--  2.0 unx    12791 b- defN 23-Aug-04 06:25 pai/huggingface/model.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-01 09:35 pai/libs/__init__.py
+-rw-r--r--  2.0 unx       22 b- defN 23-Jun-26 07:00 pai/libs/alibabacloud_aiworkspace20210204/__init__.py
+-rw-r--r--  2.0 unx   278209 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_aiworkspace20210204/client.py
+-rw-r--r--  2.0 unx   400899 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_aiworkspace20210204/models.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Jun-26 07:00 pai/libs/alibabacloud_eas20210701/__init__.py
+-rw-r--r--  2.0 unx   188668 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_eas20210701/client.py
+-rw-r--r--  2.0 unx   251200 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_eas20210701/models.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Mar-01 09:35 pai/libs/alibabacloud_pai_dlc20201203/__init__.py
+-rw-r--r--  2.0 unx    73033 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_pai_dlc20201203/client.py
+-rw-r--r--  2.0 unx   176083 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_pai_dlc20201203/models.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Mar-01 09:35 pai/libs/alibabacloud_paiflow20210202/__init__.py
+-rw-r--r--  2.0 unx    76356 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_paiflow20210202/client.py
+-rw-r--r--  2.0 unx   116997 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_paiflow20210202/models.py
+-rw-r--r--  2.0 unx       22 b- defN 23-Jun-29 11:19 pai/libs/alibabacloud_paistudio20220112/__init__.py
+-rw-r--r--  2.0 unx   107153 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_paistudio20220112/client.py
+-rw-r--r--  2.0 unx   236081 b- defN 23-Aug-02 09:49 pai/libs/alibabacloud_paistudio20220112/models.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:00 pai/modelscope/__init__.py
+-rw-r--r--  2.0 unx     9985 b- defN 23-Jul-13 15:45 pai/modelscope/estimator.py
+-rw-r--r--  2.0 unx    12704 b- defN 23-Jun-26 07:00 pai/modelscope/model.py
+-rw-r--r--  2.0 unx      503 b- defN 23-Jun-26 07:00 pai/pipeline/__init__.py
+-rw-r--r--  2.0 unx     2463 b- defN 23-Mar-01 09:35 pai/pipeline/artifact.py
+-rw-r--r--  2.0 unx       81 b- defN 23-Mar-01 09:35 pai/pipeline/consts.py
+-rw-r--r--  2.0 unx    13317 b- defN 23-Aug-04 06:25 pai/pipeline/core.py
+-rw-r--r--  2.0 unx    15710 b- defN 23-Jun-26 07:00 pai/pipeline/run.py
+-rw-r--r--  2.0 unx    12848 b- defN 23-Mar-01 09:35 pai/pipeline/step.py
+-rw-r--r--  2.0 unx      128 b- defN 23-Jun-26 07:00 pai/pipeline/component/__init__.py
+-rw-r--r--  2.0 unx     9801 b- defN 23-Apr-07 06:06 pai/pipeline/component/_base.py
+-rw-r--r--  2.0 unx     2681 b- defN 23-Mar-01 09:35 pai/pipeline/component/_container.py
+-rw-r--r--  2.0 unx     8522 b- defN 23-Jun-26 07:00 pai/pipeline/component/_registered.py
+-rw-r--r--  2.0 unx      667 b- defN 23-Mar-01 09:35 pai/pipeline/types/__init__.py
+-rw-r--r--  2.0 unx    25584 b- defN 23-Aug-04 06:25 pai/pipeline/types/artifact.py
+-rw-r--r--  2.0 unx     9725 b- defN 23-Mar-01 09:35 pai/pipeline/types/parameter.py
+-rw-r--r--  2.0 unx     6055 b- defN 23-Mar-01 09:35 pai/pipeline/types/spec.py
+-rw-r--r--  2.0 unx     4249 b- defN 23-Mar-01 09:35 pai/pipeline/types/variable.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:00 pai/schema/__init__.py
+-rw-r--r--  2.0 unx     2871 b- defN 23-Mar-01 09:35 pai/schema/base.py
+-rw-r--r--  2.0 unx     2893 b- defN 23-Jul-27 09:37 pai/schema/training_job_schema.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:00 pai/toolkit/__init__.py
+-rw-r--r--  2.0 unx    16185 b- defN 23-Jun-26 07:00 pai/toolkit/config.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:00 pai/toolkit/helper/__init__.py
+-rw-r--r--  2.0 unx     2559 b- defN 23-Jun-26 07:00 pai/toolkit/helper/consts.py
+-rw-r--r--  2.0 unx    10835 b- defN 23-Jun-26 07:00 pai/toolkit/helper/utils.py
+-rw-r--r--  2.0 unx    11358 b- defN 23-Aug-04 06:33 alipai-0.4.2.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     1255 b- defN 23-Aug-04 06:33 alipai-0.4.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-04 06:33 alipai-0.4.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Aug-04 06:33 alipai-0.4.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6968 b- defN 23-Aug-04 06:33 alipai-0.4.2.dist-info/RECORD
+83 files, 2470704 bytes uncompressed, 251749 bytes compressed:  89.8%
```

## zipnote {}

```diff
@@ -1,10 +1,7 @@
-Filename: pai/VERSION
-Comment: 
-
 Filename: pai/__init__.py
 Comment: 
 
 Filename: pai/estimator.py
 Comment: 
 
 Filename: pai/exception.py
@@ -21,14 +18,17 @@
 
 Filename: pai/serializers.py
 Comment: 
 
 Filename: pai/session.py
 Comment: 
 
+Filename: pai/tensorboard.py
+Comment: 
+
 Filename: pai/api/__init__.py
 Comment: 
 
 Filename: pai/api/algorithm.py
 Comment: 
 
 Filename: pai/api/api_container.py
@@ -63,14 +63,17 @@
 
 Filename: pai/api/pipeline_run.py
 Comment: 
 
 Filename: pai/api/service.py
 Comment: 
 
+Filename: pai/api/tensorboard.py
+Comment: 
+
 Filename: pai/api/training_job.py
 Comment: 
 
 Filename: pai/api/workspace.py
 Comment: 
 
 Filename: pai/common/__init__.py
@@ -225,23 +228,23 @@
 
 Filename: pai/toolkit/helper/consts.py
 Comment: 
 
 Filename: pai/toolkit/helper/utils.py
 Comment: 
 
-Filename: alipai-0.4.1.dist-info/LICENSE.txt
+Filename: alipai-0.4.2.dist-info/LICENSE.txt
 Comment: 
 
-Filename: alipai-0.4.1.dist-info/METADATA
+Filename: alipai-0.4.2.dist-info/METADATA
 Comment: 
 
-Filename: alipai-0.4.1.dist-info/WHEEL
+Filename: alipai-0.4.2.dist-info/WHEEL
 Comment: 
 
-Filename: alipai-0.4.1.dist-info/top_level.txt
+Filename: alipai-0.4.2.dist-info/top_level.txt
 Comment: 
 
-Filename: alipai-0.4.1.dist-info/RECORD
+Filename: alipai-0.4.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pai/__init__.py

```diff
@@ -1,14 +1,15 @@
-from __future__ import absolute_import
+import sys
 
-import os
-
-
-def read_version():
-    version_file = os.path.join(os.path.dirname(__file__), "VERSION")
-
-    if os.path.exists(version_file):
-        with open(version_file, "r") as f:
-            return f.read().strip()
-
-
-__version__ = read_version()
+if sys.version_info >= (3, 8):
+    # noinspection PyCompatibility
+    from importlib.metadata import PackageNotFoundError, version
+else:
+    from importlib_metadata import PackageNotFoundError, version
+
+PACKAGE_NAME = "alipai"
+
+try:
+    __version__ = version(PACKAGE_NAME)
+except PackageNotFoundError:
+    # package is not installed
+    __version__ = None
```

## pai/estimator.py

```diff
@@ -1,7 +1,8 @@
+import copy
 import distutils.dir_util
 import json
 import logging
 import os
 import posixpath
 import re
 import shlex
@@ -9,37 +10,37 @@
 import tempfile
 import textwrap
 import time
 from concurrent.futures import ThreadPoolExecutor
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Union
 
-import six
-
-from .api.entity_base import EntityBaseMixin
-from .common import ProviderAlibabaPAI, git_utils
-from .common.consts import INSTANCE_TYPE_LOCAL_GPU, JobType
-from .common.docker_utils import run_container
-from .common.oss_utils import OssUriObj, download, is_oss_uri, upload
-from .common.utils import is_local_run_instance_type, random_str, to_plain_text
-from .model import InferenceSpec, Model, ResourceConfig
-from .predictor import Predictor
-from .schema.training_job_schema import TrainingJobSchema
-from .serializers import SerializerBase
-from .session import Session, config_default_session, get_default_session
+from pai.api.entity_base import EntityBaseMixin
+from pai.common import ProviderAlibabaPAI, git_utils
+from pai.common.consts import INSTANCE_TYPE_LOCAL_GPU, JobType
+from pai.common.docker_utils import ContainerRun, run_container
+from pai.common.oss_utils import OssUriObj, download, is_oss_uri, upload
+from pai.common.utils import (
+    is_local_run_instance_type,
+    make_list_resource_iterator,
+    random_str,
+    to_plain_text,
+)
+from pai.exception import UnexpectedStatusException
+from pai.model import InferenceSpec, Model, ResourceConfig
+from pai.predictor import Predictor
+from pai.schema.training_job_schema import TrainingJobSchema
+from pai.serializers import SerializerBase
+from pai.session import Session, config_default_session, get_default_session
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_OUTPUT_MODEL_CHANNEL_NAME = "model"
 DEFAULT_CHECKPOINT_CHANNEL_NAME = "checkpoints"
-
-_TRAINING_JOB_URL_PATTERN = (
-    "https://pai.console.aliyun.com/?regionId={region_id}"
-    "&workspaceId={workspace_id}#/training/jobs/{job_id}/configs"
-)
+DEFAULT_LOGS_CHANNEL_NAME = "logs"
 
 
 class Estimator(object):
     """The Estimator object is responsible for submitting TrainingJob.
 
     The Estimator helps to run a training script in the PAI Training Service with a
     specific image.
@@ -206,14 +207,16 @@
                         },
                         {
                             "Name": "train-accuracy",
                             "Regex": r".*validation_0-auc="
                                      r"([-+]?[0-9]*.?[0-9]+(?:[eE][-+]?[0-9]+)?).*",
                         },
                     ]
+            session (Session, optional): A PAI session instance used for communicating
+                with PAI service.
 
         """
         self.image_uri = image_uri
         self.command = command
         self.source_dir = source_dir
         self.git_config = git_config
         self.hyperparameters = hyperparameters or dict()
@@ -254,16 +257,16 @@
 
     def _check_instance_type(self):
         """Check if the given instance_type is supported for training job."""
         if not is_local_run_instance_type(
             self.instance_type
         ) and not self.session.is_supported_training_instance(self.instance_type):
             raise ValueError(
-                f"Instance type {self.instance_type} not supproted. "
-                "Please provide a supported instance type to create the job."
+                f"Instance type='{self.instance_type}' is not supported."
+                " Please provide a supported instance type to create the job."
             )
 
     def _upload_source_files(self, job_name: str) -> Optional[str]:
         """Upload local source files to OSS."""
         if not self.source_dir:
             return
 
@@ -297,37 +300,47 @@
             "Command": command,
             "Image": self.training_image_uri(),
             "JobType": self.job_type,
             "MetricDefinitions": [m for m in self.metric_definitions]
             if self.metric_definitions
             else [],
             "CodeDir": code_input,
+            "OutputChannels": [
+                {
+                    "Name": "logs",
+                    "Properties": {
+                        "ossAppendable": "true",
+                    },
+                }
+            ],
         }
         return algo_spec
 
-    @classmethod
     def _build_input_data_configs(
-        cls, inputs: Dict[str, Any] = None
+        self, inputs: Dict[str, Any] = None
     ) -> List[Dict[str, str]]:
         inputs = inputs or dict()
         res = []
         for name, item in inputs.items():
-            if isinstance(item, six.string_types):
-                # TODO: check input URI schema && support dataset_id as training input.
-                res.append(
-                    {
-                        "Name": name,
-                        "InputUri": item,
-                    }
+            if not isinstance(item, str):
+                raise ValueError(
+                    f"The Estimator supports OSS URI or NAS URI as input data."
+                    f" Input data of type {type(item)} is not supported."
                 )
+            if is_oss_uri(item):
+                input_uri = item
+            elif os.path.exists(item):
+                store_path = self.session.get_storage_path_by_category("train_data")
+                input_uri = upload(item, store_path)
             else:
                 raise ValueError(
-                    "The Estimator supports OSS URI or NAS URI as input data, "
-                    f"Input data of type {type(item)} is not supported."
+                    f"Input data path should be a valid OSS URI or local path:"
+                    f" input={item}"
                 )
+            res.append({"Name": name, "InputUri": input_uri})
 
         return res
 
     def _build_output_data_configs(self, job_name: str) -> List[Dict[str, str]]:
         job_base_output_path = self._generate_job_base_output_path(job_name)
 
         # OSS URI for output channel will be mounted to directory
@@ -341,23 +354,33 @@
         )
         # Use checkpoints_path from user or construct a checkpoint path using
         # default output path.
         checkpoints_path = self.checkpoints_path or posixpath.join(
             job_base_output_path,
             DEFAULT_CHECKPOINT_CHANNEL_NAME,
         )
+
+        # Output logs path
+        logs_path = posixpath.join(
+            job_base_output_path,
+            DEFAULT_LOGS_CHANNEL_NAME,
+        )
         res = [
             {
                 "Name": DEFAULT_OUTPUT_MODEL_CHANNEL_NAME,
                 "OutputUri": as_oss_dir_uri(model_path),
             },
             {
                 "Name": DEFAULT_CHECKPOINT_CHANNEL_NAME,
                 "OutputUri": as_oss_dir_uri(checkpoints_path),
             },
+            {
+                "Name": DEFAULT_LOGS_CHANNEL_NAME,
+                "OutputUri": as_oss_dir_uri(logs_path),
+            },
         ]
         return res
 
     def _build_code_input(self, job_name: str) -> Optional[Dict[str, Any]]:
         """Build a dict to represent AlgorithmSpecCodeDir used in the TrainingJob."""
         upload_source_files = self._upload_source_files(job_name)
         if not upload_source_files:
@@ -398,69 +421,133 @@
         find the image to use for model training.
 
         Returns:
             str: The URI of the Docker image.
         """
         return self.image_uri
 
-    def fit(self, inputs: Dict[str, Any] = None, wait=True):
+    @property
+    def latest_training_job(self):
+        """Return the latest submitted training job."""
+        return self._latest_training_job
+
+    def fit(
+        self, inputs: Dict[str, Any] = None, wait: bool = True, show_logs: bool = True
+    ):
         """Submit a training job with the given input data.
 
         Args:
             inputs (Dict[str, Any]): A dictionary representing the input data for the
                 training job. Each key/value pair in the dictionary is an input channel,
                 the key is the channel name, and the value is the input data. The input
                 data can be an OSS URI or a NAS URI object and will be mounted to the
                 `/ml/input/data/{channel_name}` directory in the training container.
             wait (bool): Specifies whether to block until the training job is completed,
                 either succeeded, failed, or stopped. (Default True).
+            show_logs (bool): Specifies whether to show the logs produced by the
+                training job (Default True).
+        Raises:
+            UnExpectedStatusException: If the training job fails.
+
         """
         inputs = inputs or dict()
         self._prepare_for_training()
         job_name = self._gen_job_display_name()
         if is_local_run_instance_type(self.instance_type):
             training_job = self._local_run(
                 job_name=job_name, inputs=inputs, instance_type=self.instance_type
             )
         else:
             training_job = self._fit(inputs=inputs, job_name=job_name)
         self._latest_training_job = training_job
 
         if wait:
-            self._latest_training_job.wait_for_completion()
+            self.wait(show_logs=show_logs)
+
+    def wait(self, show_logs: bool = True):
+        """Block until the latest training job is completed.
+
+        Args:
+            show_logs(bool): Specifies whether to fetch and print the logs produced by
+                the training job.
+
+        """
+        if not self._latest_training_job:
+            raise RuntimeError("Could not find a submitted training job.")
+        self._latest_training_job.wait(show_logs=show_logs)
+
+    def tensorboard(self, wait=True):
+        """Launch a TensorBoard Application to view the output TensorBoard logs.
+
+        Args:
+            wait (bool): Specifies whether to block until the TensorBoard is running.
+
+        Returns:
+            :class:`pai.tensorboard.TensorBoard`: A TensorBoard instance.
+        """
+        from pai.tensorboard import TensorBoard
+
+        if not self.latest_training_job:
+            raise RuntimeError("Could not find a submitted training job.")
+
+        if isinstance(self.latest_training_job, _LocalTrainingJob):
+            raise RuntimeError("Local training job does not support tensorboard.")
+        res = self.session.tensorboard_api.list(
+            source_type="TrainingJob",
+            source_id=self.latest_training_job.training_job_id,
+        )
+
+        if res.items:
+            if len(res.items) > 1:
+                logger.warning(
+                    "Found multiple TensorBoard instances, use the first one."
+                )
+            tb_id = res.items[0]["TensorboardId"]
+            tb = TensorBoard(tensorboard_id=tb_id, session=self.session)
+            tb.start(wait=wait)
+        else:
+            tb = TensorBoard.create(
+                uri=self.logs_data(),
+                wait=wait,
+                display_name=self._latest_training_job.training_job_name,
+                source_id=self.latest_training_job.training_job_id,
+                source_type="TrainingJob",
+                session=self.session,
+            )
+        return tb
 
     def _fit(self, job_name, inputs: Dict[str, Any] = None):
         input_configs = self._build_input_data_configs(inputs)
         for c in input_configs:
             if "InputUri" in c and is_oss_uri(c["InputUri"]):
                 c["InputUri"] = self._patch_default_oss_endpoint(c["InputUri"])
         output_configs = self._build_output_data_configs(job_name)
         for c in output_configs:
             if "OutputUri" in c and is_oss_uri(c["OutputUri"]):
                 c["OutputUri"] = self._patch_default_oss_endpoint(c["OutputUri"])
 
         # prepare input code.
         code_input = self._build_code_input(job_name)
-        algo_spec = self._build_algorithm_spec(code_input=code_input)
+        algo_spec = self._build_algorithm_spec(
+            code_input=code_input,
+        )
 
         training_job_id = self.session.training_job_api.create(
             instance_count=self.instance_count,
             instance_type=self.instance_type,
             job_name=job_name,
             hyperparameters=self.hyperparameters,
             max_running_in_seconds=self.max_run_time,
             input_channels=input_configs,
             output_channels=output_configs,
             algorithm_spec=algo_spec,
         )
         training_job = _TrainingJob.get(training_job_id)
         print(
-            "View the job detail by accessing the console URI: {}".format(
-                training_job.console_uri
-            )
+            f"View the job detail by accessing the console URI: {training_job.console_uri}"
         )
         return training_job
 
     def _patch_default_oss_endpoint(self, uri: str):
         """Patch default OSS endpoint for Input/Output OSS data uri for TrainingJob."""
         return self.session.patch_oss_endpoint(uri)
 
@@ -515,14 +602,29 @@
             raise RuntimeError(
                 "No TrainingJob for the estimator, output checkpoints data not found."
             )
         return self._latest_training_job.output_path(
             channel_name=DEFAULT_CHECKPOINT_CHANNEL_NAME
         )
 
+    def logs_data(self) -> str:
+        """Output logs path.
+
+        Returns:
+            str: A string in OSS URI format refers to the checkpoints of submitted
+                training job.
+        """
+        if not self._latest_training_job:
+            raise RuntimeError(
+                "No TrainingJob for the estimator, output logs data not found."
+            )
+        return self._latest_training_job.output_path(
+            channel_name=DEFAULT_LOGS_CHANNEL_NAME,
+        )
+
     def create_model(self, inference_spec: Union[InferenceSpec, Dict]) -> Model:
         """Create a Model object using output model of the training job.
 
         Args:
             inference_spec (InferenceSpec): A ``InferenceSpec`` instance that describe
              how to create a prediction service with the output model.
 
@@ -561,18 +663,18 @@
             instance_type (str, optional): The machine instance type for the service.
             instance_count (int): Number of machine instance count.
             resource_config (Union[ResourceConfig, Dict[str, int]], optional): Resource
                 config for each prediction service instance.
             resource_id (str, optional): The ID of the resource group. If not provided,
                 the prediction service is deployed to ``public resource group``.
             serializer (SerializerBase): A SerializerBase instance used to serialize
-                the prediction reqeust data and deserialize the response data.
+                the prediction request data and deserialize the response data.
             options (Dict[str, Any], optional): Additional options for the prediction
                 service.
-            wait (bool): If ture, wait until the service is ready (Default True).
+            wait (bool): If true, wait until the service is ready (Default True).
 
         Returns:
             :class:`pai.predictor.Predictor`: A predictor instance refers to the created
                 prediction service.
         """
         m = Model(
             model_data=self.model_data(),
@@ -588,14 +690,592 @@
             serializer=serializer,
             options=options,
             wait=wait,
         )
         return p
 
 
+class AlgorithmEstimator(Estimator):
+    """Handle training jobs with algorithms
+
+    The AlgorithmEstimator provides a simple way for submitting training jobs with
+    algorithms.
+
+    Example::
+
+        # Create an AlgorithmEstimator with built-in algorithms
+        est = AlgorithmEstimator(
+            algorithm_name="pai-algorithm-test",
+            algorithm_version="0.1.0",
+            algorithm_provider="pai",
+        )
+
+        # Inspect the definition of hyperparameters, input channels and output channels
+        print(est.hyperparameters_definition)
+        print(est.input_channels_definition)
+        print(est.output_channels_definition)
+        print(est.supported_instance_types)
+
+        # Submit a training job
+        est.fit(
+            inputs={
+                "train": "oss://bucket/path/to/train/data",
+                "test": "oss://bucket/path/to/test/data",
+            },
+        )
+
+        # Inspect all outputs data
+        print(est.get_outputs_data())
+
+    """
+
+    def __init__(
+        self,
+        algorithm_name: Optional[str] = None,
+        algorithm_version: Optional[str] = None,
+        algorithm_provider: Optional[str] = None,
+        algorithm_spec: Optional[Dict[str, Any]] = None,
+        hyperparameters: Optional[Dict[str, Any]] = None,
+        base_job_name: Optional[str] = None,
+        max_run_time: Optional[int] = None,
+        output_path: Optional[str] = None,
+        instance_type: Optional[str] = None,
+        instance_count: Optional[int] = None,
+        session: Optional[Session] = None,
+    ):
+        """Initialize an AlgorithmEstimator.
+
+        Args:
+            algorithm_name (str, optional): The name of the registered algorithm. If not
+                provided, the algorithm_spec must be provided.
+            algorithm_version (str, optional): The version of the algorithm. If not
+                provided, the latest version of the algorithm will be used. If algorithm
+                name is not provided, this argument will be ignored.
+            algorithm_provider (str, optional): The provider of the algorithm. Currently
+                only "pai" or None are supported. Set it to "pai" to retrieve a PAI
+                official algorithm. If not provided, the default provider is user's PAI
+                account. If algorithm name is not provided, this argument will be
+                ignored.
+            algorithm_spec (Dict[str, Any], optional): A temporary algorithm spec.
+                Required if algorithm_name is not provided.
+            hyperparameters (dict, optional): A dictionary that represents the
+                hyperparameters used in the training job. Default hyperparameters will
+                be retrieved from the algorithm definition. The hyperparameters will be
+                stored in the `/ml/input/config/hyperparameters.json` as a JSON
+                dictionary in the training container.
+            base_job_name (str, optional): The base name used to generate the training
+                job name. If not provided, a default job name will be generated.
+            max_run_time (int, optional): The maximum time in seconds that the training
+                job can run. The training job will be terminated after the time is
+                reached (Default None).
+            output_path (str, optional): An OSS URI to store the outputs of the training
+                jobs. If not provided, an OSS URI will be generated using the default
+                OSS bucket in the session. When the `estimator.fit` method is called,
+                a specific OSS URI under the output_path for each channel is generated
+                and mounted to the training container.
+
+                A completed training container directory structure example::
+
+                    /ml
+                    |-- usercode            			// User source code directory.
+                    |   |-- requirements.txt
+                    |   `-- train.py
+                    |-- input               			// TrainingJob input
+                    |   `-- config
+                    |       |-- hyperparameters.json	// Hyperparameters in JSON
+                    |       |                           // dictionary format for the
+                    |       |                           // TrainingJob
+                    |       |
+                    |   `-- data            			// TrainingJob input channels
+                    |       |                           // `/ml/input/data/` is a input
+                    |       |                           // channel, and the directory
+                    |       |                           // name is the channel name.
+                    |       |                           // Each directory under the
+                    |       |-- test-data
+                    |       |   `-- test.csv
+                    |       `-- train-data
+                    |           `-- train.csv
+                    `-- output              			// TrainingJob output channels.
+                            |                           // Each directory under the
+                            |                           // `/ml/output/` is an output
+                            |                           // channel, and the directory
+                            |                           // name is the channel name.
+                            `-- model
+                            `-- checkpoints
+
+            instance_type (str, optional): The machine instance type used to run the
+                training job. If not provider, the default instance type will be
+                retrieved from the algorithm definition. To view the supported machine
+                instance types, please refer to the document:
+                https://help.aliyun.com/document_detail/171758.htm#section-55y-4tq-84y.
+            instance_count (int, optional): The number of machines used to run the
+                training job. If not provider, the default instance count will be
+                retrieved from the algorithm definition.
+            session (:class:`pai.session.Session`, optional): A PAI session object
+                used for interacting with PAI Service.
+        """
+        self._check_args(
+            algorithm_name=algorithm_name,
+            algorithm_spec=algorithm_spec,
+        )
+
+        self.session = session or get_default_session()
+
+        # Use _algo_spec to store the algorithm spec for inner use no matter the
+        # algorithm_name is provided or the algorithm_spec is provided.
+        # If algorithm_name is provided, retrieve the algorithm spec from the registry.
+        if algorithm_name:
+            _algo_version = self._get_algo_version(
+                algorithm_name=algorithm_name,
+                algorithm_version=algorithm_version,
+                algorithm_provider=algorithm_provider,
+            )
+            self._algo_spec = _algo_version["AlgorithmSpec"]
+            self.algorithm_name = _algo_version["AlgorithmName"]
+            self.algorithm_version = _algo_version["AlgorithmVersion"]
+            self.algorithm_provider = _algo_version["AlgorithmProvider"]
+            self.algorithm_spec = None
+        # If algorithm_name is not provided, use the provided algorithm_spec.
+        else:
+            self._algo_spec = algorithm_spec
+            self.algorithm_name = None
+            self.algorithm_version = None
+            self.algorithm_provider = None
+            self.algorithm_spec = algorithm_spec
+
+        self.hyperparameters = self._get_hyperparameters(hyperparameters)
+        self.base_job_name = base_job_name
+        self.max_run_time = max_run_time
+        self.output_path = output_path
+        self.instance_type = (
+            instance_type
+            if instance_type
+            else self._get_default_training_instance_type()
+        )
+        self.instance_count = instance_count if instance_count else 1
+
+        self._latest_training_job = None
+        self._output_channels = None
+
+        self._check_instance_type()
+
+    @property
+    def hyperparameters_definition(self) -> List[Dict[str, Any]]:
+        """Get the hyperparameters definition from the algorithm spec."""
+        res = (
+            self._algo_spec["HyperParameters"]
+            if "HyperParameters" in self._algo_spec
+            else []
+        )
+        return res
+
+    @property
+    def input_channels_definition(self) -> List[Dict[str, Any]]:
+        """Get the input channels definition from the algorithm spec."""
+        res = (
+            self._algo_spec["InputChannels"]
+            if "InputChannels" in self._algo_spec
+            else []
+        )
+        return res
+
+    @property
+    def output_channels_definition(self) -> List[Dict[str, Any]]:
+        """Get the output channels definition from the algorithm spec."""
+        res = (
+            self._algo_spec["OutputChannels"]
+            if "OutputChannels" in self._algo_spec
+            else []
+        )
+        return res
+
+    @property
+    def supported_instance_types(self) -> List[str]:
+        """Get the supported instance types from the algorithm spec."""
+        res = (
+            self._algo_spec["SupportedInstanceTypes"]
+            if "SupportedInstanceTypes" in self._algo_spec
+            else []
+        )
+        return res
+
+    def _check_args(
+        self,
+        algorithm_name: str,
+        algorithm_spec: Dict[str, Any],
+    ):
+        """Check the algorithm_name and algorithm_spec.
+
+        If neither algorithm_name nor algorithm_spec is provided, raise a ValueError.
+        If both algorithm_name and algorithm_spec are provided, use the algorithm_name
+        by default and ignore the algorithm_spec.
+
+        Args:
+            algorithm_name (str): The name of the algorithm.
+            algorithm_spec (dict): The algorithm spec.
+        """
+        if not algorithm_name and not algorithm_spec:
+            raise ValueError(
+                "Either algorithm_name or algorithm_spec should be provided."
+            )
+        if algorithm_name and algorithm_spec:
+            logger.warning(
+                "Both a tuple of (algorithm_name, algorithm_version,"
+                " algorithm_provider) and algorithm_spec are provided. Use the tuple of"
+                " (algorithm_name, algorithm_version, algorithm_provider) by default."
+                " The provided algorithm_spec will be ignored."
+            )
+
+    def _check_instance_type(self):
+        """Check if the given instance_type is supported for training job."""
+        if not self.session.is_supported_training_instance(self.instance_type):
+            raise ValueError(
+                f"Instance type='{self.instance_type}' is not supported."
+                " Please provide a supported instance type to create the job."
+            )
+
+    def _get_algo_version(
+        self,
+        algorithm_name: str,
+        algorithm_version: Optional[str] = None,
+        algorithm_provider: Optional[str] = None,
+    ) -> Dict[str, Any]:
+        """Get the algorithm version object.
+
+        Args:
+            algorithm_name (str): The name of the algorithm.
+            algorithm_version (str, optional): The version of the algorithm.
+            algorithm_provider (str, optional): The provider of the algorithm.
+
+        Returns:
+            dict: A dict that represents algorithm version object.
+        """
+        if not algorithm_name:
+            raise ValueError(
+                "Parameter algorithm_name cannot be None or empty. Please provide a"
+                " valid algorithm name."
+            )
+
+        resp_list_algo = self.session.algorithm_api.list(
+            algorithm_name=algorithm_name,
+            algorithm_provider=algorithm_provider,
+        )
+        if resp_list_algo.total_count == 0:
+            raise ValueError(
+                f"Could not find any algorithm with the specific"
+                f" name='{algorithm_name}' and provider='{algorithm_provider}'."
+                f" Please check the arguments."
+            )
+        algo_obj = resp_list_algo.items[0]
+        algorithm_id = algo_obj["AlgorithmId"]
+
+        if not algorithm_version:
+            resp_list_algo_versions = self.session.algorithm_api.list_versions(
+                algorithm_id=algorithm_id,
+            )
+            algo_version_obj = resp_list_algo_versions.items[-1]
+            algorithm_version = algo_version_obj["AlgorithmVersion"]
+            logger.warning(
+                f"Parameter algorithm_version is not provided, the latest"
+                f" version='{algorithm_version}' of the algorithm will be used."
+            )
+
+        resp_algo_version = self.session.algorithm_api.get_version(
+            algorithm_id=algorithm_id,
+            algorithm_version=algorithm_version,
+        )
+        return resp_algo_version
+
+    def _get_hyperparameters(
+        self, hyperparameters: Optional[Dict[str, Any]]
+    ) -> Dict[str, Any]:
+        """Get hyperparameters.
+
+        Get the default hyperparameters from the algorithm spec and update it with the
+        user provided hyperparameters.
+
+        Args:
+            hyperparameters (dict, optional): The user provided hyperparameters.
+
+        Returns:
+            dict: The dict of hyperparameters.
+        """
+        res = {}
+        hps_def = self.hyperparameters_definition
+        if hps_def:
+            # Get default hyperparameters.
+            for hp in hps_def:
+                res.update({hp["Name"]: hp["DefaultValue"]})
+            # Update with user provided hyperparameters.
+            for hp in hyperparameters if hyperparameters else {}:
+                if hp not in res:
+                    logger.warning(
+                        f"Hyperparameter='{hp}' is not defined in hyperparameters"
+                        f" definition. Make sure you are using the right"
+                        f" hyperparameters and check the hyperparameters_definition."
+                    )
+                    res.update({hp: hyperparameters[hp]})
+                else:
+                    res[hp] = hyperparameters[hp]
+        else:
+            # Use user provided hyperparameters.
+            res = hyperparameters
+        return res
+
+    def _get_default_training_instance_type(self) -> str:
+        """Get the default training instance type from the algorithm spec."""
+        instance_generator = make_list_resource_iterator(
+            self.session.job_api.list_ecs_specs
+        )
+        sup_instance_types = self.supported_instance_types
+        machine_spec = next(
+            (
+                item
+                for item in instance_generator
+                if not sup_instance_types or item["InstanceType"] in sup_instance_types
+            ),
+            None,
+        )
+        if not machine_spec:
+            raise RuntimeError(
+                "No supported training instance type found. Please check the supported"
+                " instance types."
+            )
+        return machine_spec["InstanceType"]
+
+    def _build_input_data_configs(
+        self,
+        inputs: Dict[str, Any] = None,
+    ) -> List[Dict[str, str]]:
+        """Build input data configs."""
+
+        def _check_input_uri(uri: str):
+            if not isinstance(uri, str):
+                raise ValueError(
+                    f"The Estimator supports OSS URI or NAS URI as input data."
+                    f" Input data of type{type(uri)} is not supported."
+                )
+            if is_oss_uri(uri):
+                input_uri = uri
+            elif os.path.exists(uri):
+                store_path = self.session.get_storage_path_by_category("train_data")
+                input_uri = upload(uri, store_path)
+            else:
+                raise ValueError(
+                    f"Input data path should be a valid OSS URI or local path:"
+                    f" input={uri}"
+                )
+            return input_uri
+
+        inputs = copy.deepcopy(inputs) or {}
+        res = []
+        input_channels_def = self.input_channels_definition
+        if input_channels_def:
+            for channel in input_channels_def:
+                channel_name = channel["Name"]
+                channel_required = channel["Required"]
+                if channel_required is True and channel_name not in inputs:
+                    raise ValueError(
+                        f"Input channel {channel_name} is required but not provided."
+                        " Please check the input channels definition."
+                    )
+                elif channel_name in inputs:
+                    input_uri = _check_input_uri(inputs[channel_name])
+                    res.append({"Name": channel_name, "InputUri": input_uri})
+                    inputs.pop(channel_name)
+                else:
+                    pass
+            if inputs != {}:
+                raise ValueError(
+                    f"Following input channels={inputs.keys()} are not defined in input"
+                    " channels definition. Please check the input channels definition."
+                )
+        else:
+            for name, item in inputs.items():
+                input_uri = _check_input_uri(item)
+                res.append({"Name": name, "InputUri": input_uri})
+
+        return res
+
+    def _build_output_data_configs(self, job_name: str) -> List[Dict[str, str]]:
+        """Build output data configs."""
+        job_base_output_path = self._generate_job_base_output_path(job_name)
+
+        # OSS URI for output channel will be mounted to directory
+        # "/ml/output/{ChannelName}/" and the output OSS URI should be a "directory"
+        def as_oss_dir_uri(uri: str):
+            return uri if uri.endswith("/") else uri + "/"
+
+        model_path = posixpath.join(
+            job_base_output_path,
+            DEFAULT_OUTPUT_MODEL_CHANNEL_NAME,
+        )
+        # Use checkpoints_path from user or construct a checkpoint path using default
+        # output path.
+        checkpoints_path = posixpath.join(
+            job_base_output_path,
+            DEFAULT_CHECKPOINT_CHANNEL_NAME,
+        )
+        # Output logs path
+        logs_path = posixpath.join(
+            job_base_output_path,
+            DEFAULT_LOGS_CHANNEL_NAME,
+        )
+
+        # Record the output channels
+        self._output_channels = []
+        res = []
+        output_channels_def = self.output_channels_definition
+        if output_channels_def:
+            for channel in output_channels_def:
+                channel_name = channel["Name"]
+                channel_path = posixpath.join(
+                    job_base_output_path,
+                    channel_name,
+                )
+                res.append(
+                    {
+                        "Name": channel_name,
+                        "OutputUri": as_oss_dir_uri(channel_path),
+                    }
+                )
+                self._output_channels.append(channel_name)
+        else:
+            res = [
+                {
+                    "Name": DEFAULT_OUTPUT_MODEL_CHANNEL_NAME,
+                    "OutputUri": as_oss_dir_uri(model_path),
+                },
+                {
+                    "Name": DEFAULT_CHECKPOINT_CHANNEL_NAME,
+                    "OutputUri": as_oss_dir_uri(checkpoints_path),
+                },
+                {
+                    "Name": DEFAULT_LOGS_CHANNEL_NAME,
+                    "OutputUri": as_oss_dir_uri(logs_path),
+                },
+            ]
+            self._output_channels = [
+                DEFAULT_OUTPUT_MODEL_CHANNEL_NAME,
+                DEFAULT_CHECKPOINT_CHANNEL_NAME,
+                DEFAULT_LOGS_CHANNEL_NAME,
+            ]
+
+        return res
+
+    def fit(
+        self, inputs: Dict[str, Any] = None, wait: bool = True, show_logs: bool = True
+    ):
+        """Submit a training job with the given input data.
+
+        Args:
+            inputs (Dict[str, Any]): A dictionary representing the input data for the
+                training job. Each key/value pair in the dictionary is an input channel,
+                the key is the channel name, and the value is the input data. The input
+                data can be an OSS URI or a NAS URI object and will be mounted to the
+                `/ml/input/data/{channel_name}` directory in the training container.
+            wait (bool): Specifies whether to block until the training job is completed,
+                either succeeded, failed, or stopped. (Default True).
+            show_logs (bool): Specifies whether to show the logs produced by the
+                training job (Default True).
+        Raises:
+            UnExpectedStatusException: If the training job fails.
+
+        """
+        inputs = inputs or dict()
+        job_name = self._gen_job_display_name()
+        training_job = self._fit(inputs=inputs, job_name=job_name)
+        self._latest_training_job = training_job
+
+        if wait:
+            self.wait(show_logs=show_logs)
+
+    def _fit(self, job_name, inputs: Dict[str, Any] = None):
+        input_configs = self._build_input_data_configs(inputs)
+        for c in input_configs:
+            if "InputUri" in c and is_oss_uri(c["InputUri"]):
+                c["InputUri"] = self._patch_default_oss_endpoint(c["InputUri"])
+        output_configs = self._build_output_data_configs(job_name)
+        for c in output_configs:
+            if "OutputUri" in c and is_oss_uri(c["OutputUri"]):
+                c["OutputUri"] = self._patch_default_oss_endpoint(c["OutputUri"])
+
+        training_job_id = self.session.training_job_api.create(
+            instance_count=self.instance_count,
+            instance_type=self.instance_type,
+            job_name=job_name,
+            hyperparameters=self.hyperparameters,
+            max_running_in_seconds=self.max_run_time,
+            input_channels=input_configs,
+            output_channels=output_configs,
+            algorithm_name=self.algorithm_name,
+            algorithm_version=self.algorithm_version,
+            algorithm_provider=self.algorithm_provider,
+            algorithm_spec=self.algorithm_spec,
+        )
+        training_job = _TrainingJob.get(training_job_id)
+        print(
+            f"View the job detail by accessing the console URI:"
+            f" {training_job.console_uri}"
+        )
+        return training_job
+
+    def model_data(self) -> str:
+        """Model data output path.
+
+        Returns:
+            str: A string in OSS URI format refers to the output model of the submitted
+                job.
+        """
+        if not self._latest_training_job:
+            raise RuntimeError(
+                "No TrainingJob for the estimator, output model data not found."
+            )
+
+        if not self._latest_training_job.is_succeeded():
+            logger.warning(
+                "The TrainingJob is currently not in a succeeded status, which means"
+                " that the model data output may not be accessible."
+            )
+
+        if DEFAULT_OUTPUT_MODEL_CHANNEL_NAME not in self._output_channels:
+            raise RuntimeError(
+                f"Default output model channel={DEFAULT_OUTPUT_MODEL_CHANNEL_NAME} not"
+                " found in the output channels definition. Please check the output"
+                " channels definition."
+            )
+
+        res = self._latest_training_job.output_path(
+            channel_name=DEFAULT_OUTPUT_MODEL_CHANNEL_NAME
+        )
+        return res
+
+    def get_outputs_data(self) -> Dict[str, str]:
+        """Show all outputs data paths.
+
+        Returns:
+            dict[str, str]: A dictionary of all outputs data paths.
+        """
+        if not self._latest_training_job:
+            raise RuntimeError(
+                "No TrainingJob for the estimator, output checkpoints data not found."
+            )
+
+        res = {}
+        for channel_name in self._output_channels:
+            res.update(
+                {
+                    channel_name: self._latest_training_job.output_path(
+                        channel_name=channel_name
+                    )
+                }
+            )
+        return res
+
+
 _TRAINING_LAUNCH_SCRIPT_TEMPLATE = textwrap.dedent(
     """\
 #!/bin/sh
 
 env
 
 # change to working directory
@@ -650,15 +1330,30 @@
     ):
         self.estimator = estimator
         self.inputs = inputs
         self.tmp_dir = temp_dir or tempfile.mkdtemp()
         self.job_name = job_name
         self.instance_type = instance_type
         logger.info("Local TrainingJob temporary directory: {}".format(self.tmp_dir))
-        self._container_run = None
+        self._container_run: ContainerRun = None
+
+    def __str__(self):
+        return self.__repr__()
+
+    def __repr__(self):
+        if self._container_run:
+            container = self._container_run.container
+            container_name, container_id, status = (
+                container.name,
+                container.id,
+                container.status,
+            )
+        else:
+            container_name, container_id, status = None, None, None
+        return f"LocalTrainingJob(container_name={container_name}, container_id={container_id}, status={status})"
 
     @property
     def session(self) -> Session:
         return self.estimator.session
 
     def prepare_env(self) -> Dict[str, str]:
         """Prepare environment variables for the training job."""
@@ -817,26 +1512,27 @@
                     shutil.copy(
                         input_data,
                         os.path.join(local_channel_path, os.path.basename(input_data)),
                     )
 
         return input_data_configs
 
-    def wait_for_completion(self):
-        self._container_run.watch()
+    def wait(self, show_logs: bool = True):
+        self._container_run.watch(show_logs=show_logs)
 
     def output_path(self, channel_name="model"):
         return os.path.join(self.tmp_dir, "output", f"{channel_name}/")
 
     def is_succeeded(self):
         """Return True if the training job is succeeded, otherwise return False."""
         return self._container_run.is_succeeded()
 
 
 class TrainingJobStatus(object):
+    CreateFailed = "CreateFailed"
     InitializeFailed = "InitializeFailed"
     Succeed = "Succeed"
     Failed = "Failed"
     Terminated = "Terminated"
     Creating = "Creating"
     Created = "Created"
     Initializing = "Initializing"
@@ -853,15 +1549,15 @@
         ]
 
     @classmethod
     def failed_status(cls):
         return [
             cls.InitializeFailed,
             cls.Failed,
-            cls.Terminated,
+            cls.CreateFailed,
         ]
 
 
 class TrainingJobChannel(object):
     def __init__(self, dataset_id=None, input_uri=None, name=None):
         self.dataset_id = dataset_id
         self.input_uri = input_uri
@@ -907,14 +1603,21 @@
         self.create_time = kwargs.pop("create_time", None)
         self.modified_time = kwargs.pop("modified_time", None)
         self.reason_code = kwargs.pop("reason_code", None)
         self.reason_message = kwargs.pop("reason_message", None)
         self.status = kwargs.pop("status", None)
         self.status_transitions = kwargs.pop("status_transitions", None)
         self.training_job_id = kwargs.pop("training_job_id", None)
+        self.training_job_url = kwargs.pop("training_job_url", None)
+
+    def __repr__(self):
+        return "TrainingJob(id={})".format(self.training_job_id)
+
+    def __str__(self):
+        return self.__repr__()
 
     @property
     def id(self):
         return self.training_job_id
 
     @classmethod
     @config_default_session
@@ -945,100 +1648,136 @@
         )
 
     @property
     def console_uri(self):
         if not self.training_job_id:
             raise ValueError("The TrainingJob is not submitted")
 
-        return _TRAINING_JOB_URL_PATTERN.format(
-            region_id=self.session.region_id,
-            workspace_id=self.session.workspace_id,
-            job_id=self.training_job_id,
-        )
+        return self.training_job_url
 
-    def wait_for_completion(self, interval=2):
+    def wait(self, interval=2, show_logs: bool = True):
         self.session.training_job_api.refresh_entity(self.training_job_id, self)
-        job_logger = _TrainingJobLogger(training_job=self, page_size=20)
-        future = job_logger.start()
+
+        if show_logs:
+            job_log_printer = _TrainingJobLogPrinter(
+                training_job_id=self.training_job_id, page_size=20, session=self.session
+            )
+            job_log_printer.start()
+        else:
+            job_log_printer = None
         try:
             while self.status not in TrainingJobStatus.completed_status():
                 time.sleep(interval)
                 self.session.training_job_api.refresh_entity(self.training_job_id, self)
         finally:
-            job_logger.stop()
-            future.result()
+            if job_log_printer:
+                job_log_printer.stop(wait=True)
 
         self._on_job_completed()
 
     def _on_job_completed(self):
-        # print an empty line to separate the training job logs and the following logs
+        # Print an empty line to separate the training job logs and the following logs
         print()
         if self.status == TrainingJobStatus.Succeed:
             print(
                 f"Training job ({self.training_job_id}) succeeded, you can check the"
                 f" logs/metrics/output in  the console:\n{self.console_uri}"
             )
-            return
+        elif self.status == TrainingJobStatus.Terminated:
+            print(
+                f"Training job is ended with status {self.status}: "
+                f"reason_code={self.reason_code}, reason_message={self.reason_message}."
+                f"Check the training job in the console:\n{self.console_uri}"
+            )
         elif self.status in TrainingJobStatus.failed_status():
             print(
                 f"Training job ({self.training_job_id}) failed, please check the logs"
                 f" in the console: \n{self.console_uri}"
             )
-            raise RuntimeError(
-                f"TrainingJob failed: name={self.training_job_name}, "
-                f"training_job_id={self.training_job_id}, "
-                f"reason_code={self.reason_code}, status={self.status}, "
-                f"reason_message={self.reason_message}",
-            )
+
+            message = f"TrainingJob failed: name={self.training_job_name}, "
+            f"training_job_id={self.training_job_id}, "
+            f"reason_code={self.reason_code}, status={self.status}, "
+            f"reason_message={self.reason_message}"
+
+            raise UnexpectedStatusException(message=message, status=self.status)
 
     def _reload(self):
         """Reload the training job from the PAI Service,"""
         self.session.training_job_api.refresh_entity(self.training_job_id, self)
 
     def is_succeeded(self):
         """Return True if the training job is succeeded"""
         self._reload()
         return self.status == TrainingJobStatus.Succeed
 
 
-class _TrainingJobLogger(object):
-    """TrainingJobLogger instance used to print logs for a training job"""
+class _TrainingJobLogPrinter(object):
+    """A class used to print logs for a training job"""
 
     executor = ThreadPoolExecutor(5)
 
-    def __init__(self, training_job: _TrainingJob, page_size=10):
-        self.training_job = training_job
-        self.page_offset = 0
+    def __init__(
+        self, training_job_id: str, page_size=10, session: Optional[Session] = None
+    ):
+        self.training_job_id = training_job_id
+        self.session = session
         self.page_size = page_size
-        self._following = True
+        self._future = None
+        self._stop = False
 
     def _list_logs(self):
         page_number, page_offset = 1, 0
-        while self._following:
-            res = self.training_job.session.training_job_api.list_logs(
-                self.training_job.training_job_id,
+        # print training job logs.
+        while not self._stop:
+            res = self.session.training_job_api.list_logs(
+                self.training_job_id,
                 page_number=page_number,
                 page_size=self.page_size,
             )
             # 1. move to next page
             if len(res.items) == self.page_size:
-                # print new logs
+                # print new logs starting from page_offset
                 self._print_logs(logs=res.items[page_offset:])
                 page_number += 1
                 page_offset = 0
             # 2. stay at the current page.
             else:
                 if len(res.items) > page_offset:
+                    # print new logs starting from page_offset
                     self._print_logs(logs=res.items[page_offset:])
                     page_offset = len(res.items)
                 time.sleep(1)
 
+        # When _stop is True, wait and print remaining logs.
+        time.sleep(10)
+        while True:
+            res = self.session.training_job_api.list_logs(
+                self.training_job_id,
+                page_number=page_number,
+                page_size=self.page_size,
+            )
+            # There maybe more logs in the next page
+            if len(res.items) == self.page_size:
+                self._print_logs(logs=res.items[page_offset:])
+                page_number += 1
+                page_offset = 0
+            # No more logs in the next page.
+            else:
+                if len(res.items) > page_offset:
+                    self._print_logs(logs=res.items[page_offset:])
+                break
+
     def _print_logs(self, logs: List[str]):
         for log in logs:
             print(log)
 
     def start(self):
-        self._following = True
-        return self.executor.submit(self._list_logs)
-
-    def stop(self):
-        self._following = False
+        if self._future:
+            raise ValueError("The training job log printer is already started")
+        self._stop = False
+        self._future = self.executor.submit(self._list_logs)
+
+    def stop(self, wait: bool = True):
+        self._stop = True
+        if self._future:
+            self._future.result()
```

## pai/exception.py

```diff
@@ -13,7 +13,15 @@
     def __init__(self, code, message):
         super(PredictionException, self).__init__(message)
         self.code = code
         self.message = message
 
     def __str__(self):
         return f"PredictionException: Code={self.code}, Message={self.message}"
+
+
+class UnexpectedStatusException(PAIException):
+    """Raised when resource status is not expected."""
+
+    def __init__(self, message, status):
+        self.status = status
+        super(UnexpectedStatusException, self).__init__(message)
```

## pai/model.py

```diff
@@ -326,14 +326,23 @@
                 "oss": {"path": oss_uri_obj.get_dir_uri()},
             }
         else:
             raise ValueError(
                 "Source path is not a valid OSS URI or a existing local path."
             )
 
+        # Check if the mount information is already in the config.
+        for conf in configs:
+            if conf.get("oss").get("path") == oss_uri_obj.get_dir_uri():
+                logger.warning(
+                    f"Source {oss_uri_obj.get_dir_uri()} is already mounted."
+                    " Skip mounting."
+                )
+                return None
+
         configs.append(storage_config)
         self.storage = configs
         return storage_config
 
 
 @config_default_session
 def container_serving_spec(
@@ -581,15 +590,14 @@
     @config_default_session
     def __init__(
         self,
         model_data: str,
         inference_spec: InferenceSpec,
         session: Session = None,
     ):
-
         if not model_data and "model_data" in inference_spec:
             model_data = inference_spec.model_data
         self.model_data = model_data
         self.inference_spec = inference_spec
         self.session = session
 
     def _download_model_data(self, target_dir):
@@ -663,16 +671,16 @@
             )
         else:
             return self._deploy(
                 service_name=service_name,
                 instance_count=instance_count,
                 instance_type=instance_type,
                 resource_config=resource_config,
-                service_type=service_type,
                 resource_id=resource_id,
+                service_type=service_type,
                 options=options,
                 wait=wait,
                 serializer=serializer,
             )
 
     def _generate_service_name(self):
         s = os.path.basename(self.model_data.rstrip("/")) + random_str(8)
@@ -694,16 +702,14 @@
         if not service_name:
             service_name = self._generate_service_name()
             logger.info(
                 "Service name is not specified, using a generated service"
                 f" name to create the service: service_name={service_name}"
             )
 
-        self.model_data = self._upload_model_data()
-
         config = self._build_service_config(
             service_name=service_name,
             instance_count=instance_count,
             instance_type=instance_type,
             service_type=service_type,
             resource_config=resource_config,
             resource_id=resource_id,
@@ -756,14 +762,15 @@
         instance_type: str = None,
         resource_config: Union[ResourceConfig, Dict[str, Any]] = None,
         resource_id: str = None,
         service_type: str = None,
         options: Dict[str, Any] = None,
     ) -> Dict[str, Any]:
         """Build a service config dictionary used to create a PAI EAS service."""
+        self.model_data = self._upload_model_data()
 
         resource_config = (
             ResourceConfig(**resource_config)
             if resource_config and isinstance(resource_config, dict)
             else None
         )
 
@@ -963,14 +970,123 @@
                 break
             except requests.ConnectionError:
                 # ConnectionError means server is not ready.
                 logging.debug("Waiting for the container to be ready...")
                 time.sleep(interval)
                 continue
 
+    def register(
+        self,
+        model_name: str,
+        model_labels: Optional[Dict[str, str]] = None,
+        model_description: Optional[str] = None,
+        model_doc: Optional[str] = None,
+        origin: Optional[str] = None,
+        domain: Optional[str] = None,
+        task: Optional[str] = None,
+        accessibility: Optional[str] = None,
+        version: str = None,
+        version_labels: Optional[Dict[str, str]] = None,
+        version_description: Optional[str] = None,
+        format_type: Optional[str] = None,
+        framework_type: Optional[str] = None,
+        training_spec: Optional[Dict[str, Any]] = None,
+        inference_spec: Optional[Dict[str, Any]] = None,
+        approval_status: Optional[str] = None,
+        metrics: Optional[Dict[str, Any]] = None,
+        options: Optional[str] = None,
+    ) -> "RegisteredModel":
+        """Register a model to the PAI model registry.
+
+        Use ``self.model_data`` to register a model to the PAI model registry.
+
+        Args:
+            model_name (str): The name of the model. If the model name already exists in
+                workspace, the model will be updated with a new model version,
+                parameters like ``model_labels``, ``model_description``, ``model_doc``,
+                ``origin``, ``domain``, ``task``, ``accessibility`` will be ignored. If
+                the model name does not exist, a new model will be created.
+            model_labels (dict, optional): The labels of the model.
+            model_description (str, optional): The description of the model.
+            model_doc (str, optional): The documentation uri of the model.
+            origin (str, optional): The origin of the model. For example, "huggingface",
+                "modelscope" etc. Default to None.
+            domain (str, optional): The domain that the model is used for. For example,
+                "aigc", "audio", "nlp", "cv" etc. Default to None.
+            task (str, optional): The task that the model is used for. For example,
+                "large-language-model", "text-classification", "image-classification",
+                "sequence-labeling" etc. Default to None.
+            accessibility (str, optional): The accessibility of the model. The value
+                can be "PUBLIC" or "PRIVATE". Default to "PRIVATE".
+            version (str, optional): The version of the model. If not specified, a new
+                version will be created. If the version already exists, registration
+                will fail.
+            version_labels (dict, optional): The labels of the model version.
+            version_description (str, optional): The description of the model version.
+            format_type (str, optional): The format type of the model version. The value
+                can be "OfflineModel", "SavedModel", "Keras H5", "Frozen Pb",
+                "Caffe Prototxt", "TorchScript", "XGBoost", "PMML", "AlinkModel",
+                "ONNX". Default to None.
+            framework_type (str, optional): The framework type of the model version. The
+                value can be "Pyrotch", "TensorFlow", "Keras", "Caffe", "Alink",
+                "Xflow", "XGBoost". Default to None.
+            training_spec (dict, optional): The training spec of the model version.
+                Usually, it is got from the training job. Default to None.
+            inference_spec (dict, optional): The complete inference spec of the model
+                version. Usually, it is got from the inference service. Default to None.
+            approval_status (str, optional): The approval status of the model version.
+                The value can be "APPROVED", "PENDING". Default to None.
+            metrics (dict, optional): The metrics of the model version.
+            options (str, optional): Any other options that you want to pass to the
+                model registry. Default to None.
+
+        Returns:
+            :class:`pai.model.RegisteredModel`: The registered model object.
+        """
+
+        if not self.model_data:
+            raise ValueError(
+                "Register model failed, ``model_data`` is required to register a model."
+            )
+
+        # By specifying model_name with double quotes, the list api will process the
+        # precise search. Otherwise, the list api will process the fuzzy search.
+        resp = self.session.model_api.list(
+            model_name=f'"{model_name}"',
+        )
+        if resp.total_count == 0:
+            model_id = self.session.model_api.create(
+                model_name=model_name,
+                labels=model_labels,
+                model_description=model_description,
+                model_doc=model_doc,
+                origin=origin,
+                domain=domain,
+                task=task,
+                accessibility=accessibility,
+            )
+        else:
+            model_id = resp.items[0]["ModelId"]
+
+        version_name = self.session.model_api.create_version(
+            model_id=model_id,
+            uri=self.model_data,
+            version_name=version,
+            labels=version_labels,
+            version_description=version_description,
+            format_type=format_type,
+            framework_type=framework_type,
+            training_spec=training_spec,
+            inference_spec=inference_spec,
+            approval_status=approval_status,
+            metrics=metrics,
+            options=options,
+        )
+        return RegisteredModel(model_name=model_name, model_version=version_name)
+
 
 class Model(ModelBase):
     """The Class representing a ready-to-deploy model.
 
     A Model instance includes the model artifact path and information on how to create
     prediction service in PAI (specified by the inference_spec). By calling the
     `model.deploy` method, a prediction service is created in PAI and a
@@ -985,15 +1101,15 @@
         m: Model = Model(
             inference_spec=InferenceSpec(processor="xgboost"),
             model_data="oss://bucket-name/path/to/model",
         )
 
         # register model to PAI ModelRegistry
         registered_model = m.register(
-            name="example_xgb_model"
+            model_name="example_xgb_model"
             version="1.0.0",
         )
 
         # Deploy to model to create a prediction service.
         p: Predictor = m.deploy(
             service_name="xgb_model_service",
             instance_count=2,
@@ -1046,16 +1162,16 @@
         wait: bool = True,
         serializer: Optional["SerializerBase"] = None,
         **kwargs,
     ):
         """Deploy an online prediction service.
 
         Args:
-            service_name (str, optional): Name for the online prediction service. The name
-                must be unique in a region.
+            service_name (str, optional): Name for the online prediction service. The
+                name must be unique in a region.
             instance_count (int): Number of instance request for the service deploy
                 (Default 1).
             instance_type (str, optional): Type of the machine instance, for example,
                 'ecs.c6.large'. For all supported instance, view the appendix of the
                 link:
                 https://help.aliyun.com/document_detail/144261.htm?#section-mci-qh9-4j7
             resource_config (Union[ResourceConfig, Dict[str, int]], optional):
@@ -1104,7 +1220,448 @@
             resource_id=resource_id,
             options=options,
             wait=wait,
             serializer=serializer,
             service_type=service_type,
             **kwargs,
         )
+
+
+class RegisteredModel(ModelBase):
+    """A class that represents a registered model in PAI model registry.
+
+    A RegisteredModel instance has a unique tuple of (model_name, model_version,
+    model_provider), and can be used for downstream tasks such as creating an online
+    prediction service, or creating an AlgorithmEstimator to start a training job.
+
+    Examples::
+
+        from pai.model import RegisteredModel
+
+        # retrieve a registered model from PAI model registry by
+        # specifying the model_name, model_version and model_provider
+        m = RegisteredModel(
+            model_name="easynlp_pai_bert_small_zh",
+            model_version="0.1.0",
+            model_provider="pai",
+        )
+
+
+        # deploy the Registered Model to create an online prediction
+        # service if the model has inference_spec
+        m.deploy()
+
+
+        # create an AlgorithmEstimator to start a training job if the
+        # model has training_spec
+        est = m.get_estimator()
+        inputs = m.get_estimator_inputs()
+        est.fit(inputs)
+
+    """
+
+    @config_default_session
+    def __init__(
+        self,
+        model_name: str,
+        model_version: Optional[str] = None,
+        model_provider: Optional[str] = None,
+        session: Session = None,
+    ):
+        """Get a RegisteredModel instance from PAI model registry.
+
+        Args:
+            model_name (str): The name of the registered model.
+            model_version (str, optional): The version of the registered model. If not
+                provided, the latest version is retrieved from the model registry.
+            model_provider (str, optional): The provider of the registered model.
+                Currently only "pai" or None are supported. Set it to "pai" to retrieve
+                a PAI official model. If not provided, the default provider is user's
+                PAI account.
+            session (:class:`pai.session.Session`, optional): A PAI session object used
+                for interacting with PAI Service.
+        """
+        self.session = session
+        model_version_obj = self._get_model_version_obj(
+            model_name=model_name,
+            model_version=model_version,
+            model_provider=model_provider,
+        )
+
+        self._model_id = model_version_obj.get("ModelId")
+        self.model_name = model_version_obj.get("ModelName")
+        self.model_version = model_version_obj.get("VersionName")
+        self.model_provider = model_version_obj.get("Provider")
+        self.framework_type = model_version_obj.get("FrameworkType")
+        self.format_type = model_version_obj.get("FormatType")
+        self.training_spec = model_version_obj.get("TrainingSpec")
+        self.source_type = model_version_obj.get("SourceType")
+        self.source_id = model_version_obj.get("SourceId")
+        self.labels = {
+            lb["Key"]: lb["Value"] for lb in model_version_obj.get("Labels", [])
+        }
+
+        super(RegisteredModel, self).__init__(
+            model_data=model_version_obj.get("Uri"),
+            inference_spec=InferenceSpec.from_dict(
+                model_version_obj.get("InferenceSpec", dict())
+            ),
+        )
+
+    def __eq__(self, other: "RegisteredModel") -> bool:
+        """Compare two RegisteredModel instances."""
+        return (
+            isinstance(other, RegisteredModel)
+            and other._model_id == self._model_id
+            and other.model_version == other.model_version
+        )
+
+    def _generate_service_name(self) -> str:
+        """Generate a service name for the online prediction service."""
+        base_name = self.model_name.replace("-", "_")[:36]
+        if base_name.endswith("_"):
+            base_name = base_name[:-1]
+        gen_name = f"{base_name}_{random_str(8)}"
+        return to_plain_text(gen_name)
+
+    def _get_inference_spec(self) -> InferenceSpec:
+        """Get the inference_spec of the registered model."""
+        return self.inference_spec
+
+    def _get_model_version_obj(
+        self,
+        model_name: str,
+        model_version: Optional[str] = None,
+        model_provider: Optional[str] = None,
+    ) -> Dict[str, Any]:
+        """Get the model version object from PAI model registry.
+
+        Args:
+            model_name (str): The name of the registered model.
+            model_version (str, optional): The version of the registered model. If not
+                provided, the latest version is retrieved from the model registry.
+            model_provider (str, optional): The provider of the registered model.
+                Currently only "pai" or None are supported. Set it to "pai" to retrieve
+                a PAI official model. If not provided, the default provider is user's
+                PAI account.
+
+        Returns:
+            A dict that represents the model version object.
+        """
+        if not model_name:
+            raise ValueError(
+                "Parameter model_name cannot be None or empty. Please provide a valid"
+                " model_name."
+            )
+
+        # Use model_name to get the model_id
+        # By specifying model_name with double quotes, the list api will process the
+        # precise search. Otherwise, the list api will process the fuzzy search.
+        result = self.session.model_api.list(
+            model_name=f'"{model_name}"', provider=model_provider
+        )
+        if result.total_count == 0:
+            raise RuntimeError(
+                f"Could not find any Registered Model with the specific"
+                f" name='{model_name}' and provider='{model_provider}'. Please check"
+                f" the arguments."
+            )
+        model_obj = result.items[0]
+        model_id = model_obj["ModelId"]
+
+        if model_version:
+            model_version_obj = self.session.model_api.get_version(
+                model_id=model_id, version=model_version
+            )
+            model_version_obj.update(
+                {
+                    "ModelName": model_name,
+                    "Provider": model_provider,
+                }
+            )
+        else:
+            # Get the latest model version of the specific model if model_version is not provided.
+            if "LatestVersion" not in model_obj:
+                raise RuntimeError(
+                    f"Could not find any model version under the specific"
+                    f" name='{model_name}' and provider='{model_provider}'. Please"
+                    f" check the arguments."
+                )
+            model_version_obj = model_obj["LatestVersion"]
+            model_version = model_version_obj["VersionName"]
+            model_version_obj["ModelId"] = model_id
+            model_version_obj["ModelName"] = model_name
+            model_version_obj["Provider"] = model_provider
+            logger.warning(
+                f"Parameter model_version is not provided, the latest"
+                f" version='{model_version}' of the model will be used."
+            )
+        return model_version_obj
+
+    def delete(self):
+        """Delete the specific registered model from PAI model registry"""
+        self.session.model_api.delete_version(self._model_id, self.model_version)
+
+    def deploy(
+        self,
+        service_name: Optional[str] = None,
+        instance_count: Optional[int] = None,
+        instance_type: Optional[str] = None,
+        resource_config: Optional[Union[Dict[str, int], ResourceConfig]] = None,
+        resource_id: Optional[str] = None,
+        options: Optional[Dict[str, Any]] = None,
+        service_type: Optional[str] = None,
+        wait: bool = True,
+        serializer: Optional["SerializerBase"] = None,
+        **kwargs,
+    ):
+        """Deploy an online prediction service with the registered model.
+
+        If the RegisteredModel already has a registered inference_spec, then the model
+        can be deployed directly. Give more specific arguments to override the registered
+        inference_spec. Otherwise, the model will be deployed through the same process
+        as the :meth:`deploy` method of :class:`pai.model.Model`.
+
+        Args:
+            service_name (str, optional): Name for the online prediction service. The
+                name must be unique in a region.
+            instance_count (int, optional): Number of instance requested for the service
+                deploy.
+            instance_type (str, optional): Type of the machine instance, for example,
+                'ecs.c6.large'. For all supported instance, view the appendix of the
+                link:
+                https://help.aliyun.com/document_detail/144261.htm?#section-mci-qh9-4j7
+            resource_config (Union[ResourceConfig, Dict[str, int]], optional):
+                Request resource for each instance of the service. Required if
+                instance_type is not set.  Example config:
+
+                .. code::
+
+                    resource_config = {
+                        "cpu": 2,       # The number of CPUs that each instance requires
+                        "memory: 4000,  # The amount of memory that each instance
+                                        # requires, must be an integer, Unit: MB.
+                        # "gpu": 1,         # The number of GPUs that each instance
+                                            # requires.
+                        # "gpu_memory": 3   # The amount of GPU memory that each
+                                            # instance requires, must be an integer,
+                                            # Unit: GB.
+                    }
+
+            resource_id (str, optional): The ID of the resource group. The service
+                can be deployed to ``public resource group`` and
+                ``dedicated resource group``.
+
+                * If `resource_id` is not specified, the service is deployed
+                    to public resource group.
+                * If the service deployed in a dedicated resource group, provide
+                    the parameter as the ID of the resource group. Example:
+                    "eas-r-6dbzve8ip0xnzte5rp".
+            service_type (str, optional): The type of the service.
+            options (Dict[str, Any], optional): Advanced deploy parameters used
+                to create the online prediction service.
+            wait (bool): Whether the call should wait until the online prediction
+                service is ready (Default True).
+            serializer (:class:`pai.predictor.serializers.BaseSerializer`, optional): A
+                serializer object used to serialize the prediction request and
+                deserialize the prediction response.
+        Returns:
+            A ``PredictorBase`` instance used for making prediction to the prediction
+            service.
+        """
+        if is_local_run_instance_type(instance_type):
+            return self._deploy_local(
+                instance_type=instance_type,
+                serializer=serializer,
+                wait=wait,
+            )
+        else:
+            return self._deploy(
+                service_name=service_name,
+                instance_count=instance_count,
+                instance_type=instance_type,
+                resource_config=resource_config,
+                resource_id=resource_id,
+                service_type=service_type,
+                options=options,
+                wait=wait,
+                serializer=serializer,
+            )
+
+    def _build_service_config(
+        self,
+        service_name: str = None,
+        instance_count: int = None,
+        instance_type: str = None,
+        resource_config: Union[ResourceConfig, Dict[str, Any]] = None,
+        resource_id: str = None,
+        service_type: str = None,
+        options: Dict[str, Any] = None,
+    ) -> Dict[str, Any]:
+        """Build a service config dictionary with RegisteredModel's inference_spec.
+
+        When the RegisteredModel builds a service config, it will ignore the model_data
+        parameter and use the inference_spec of the RegisteredModel as default config.
+        User can override the inference_spec by providing more specific arguments.
+        """
+
+        resource_config = (
+            ResourceConfig(**resource_config)
+            if resource_config and isinstance(resource_config, dict)
+            else None
+        )
+
+        if resource_config and instance_type:
+            raise ValueError(
+                f"Only one of 'instance_type' and 'resource_config' is required, but"
+                f" both have been provided: instance_type={instance_type},"
+                f" resource_config={resource_config}."
+            )
+
+        inference_spec = InferenceSpec(
+            self._get_inference_spec().to_dict() if self.inference_spec else dict()
+        )
+
+        if service_type:
+            inference_spec.add_option("metadata.type", service_type)
+            if inference_spec.is_container_serving():
+                inference_spec.add_option("metadata.rpc.proxy_path", "/")
+
+        if service_name:
+            inference_spec.add_option("name", service_name)
+
+        if instance_count:
+            inference_spec.add_option("metadata.instance", instance_count)
+
+        if instance_type:
+            inference_spec.add_option("cloud.computing.instance_type", instance_type)
+        elif resource_config:
+            inference_spec.add_option("metadata.cpu", resource_config.cpu)
+            inference_spec.add_option("metadata.memory", resource_config.memory)
+            if resource_config.gpu:
+                inference_spec.add_option("metadata.gpu", resource_config.gpu)
+            if resource_config.gpu_memory:
+                inference_spec.add_option(
+                    "metadata.gpu_memory", resource_config.gpu_memory
+                )
+                if resource_config.gpu:
+                    logger.warning(
+                        "Parameters 'gpu' is set, the 'gpu_memory' parameter does not"
+                        " take effect."
+                    )
+
+        if resource_id:
+            inference_spec.add_option("metadata.resource", resource_id)
+
+        if options:
+            inference_spec.merge_options(options=options)
+
+        return inference_spec.to_dict()
+
+    def get_estimator(self):
+        """Generate an AlgorithmEstimator
+
+        Generate an AlgorithmEstimator object from RegisteredModel's training_spec.
+
+        Returns:
+            :class:`pai.estimator.AlgorithmEstimator`: An AlgorithmEstimator object.
+        """
+        from .estimator import AlgorithmEstimator
+
+        if not self.training_spec:
+            raise ValueError(
+                "The provided registered model does not contain training spec."
+            )
+        ts = self.training_spec
+        if "AlgorithmSpec" not in ts and "AlgorithmName" not in ts:
+            raise ValueError(
+                "The provided registered model's training spec does not contain any"
+                " algorithms."
+            )
+
+        algorithm_name = ts["AlgorithmName"] if "AlgorithmName" in ts else None
+        algorithm_provider = (
+            ts["AlgorithmProvider"] if "AlgorithmProvider" in ts else None
+        )
+        algorithm_version = ts["AlgorithmVersion"] if "AlgorithmVersion" in ts else None
+        algorithm_spec = ts["AlgorithmSpec"] if "AlgorithmSpec" in ts else None
+
+        if "HyperParameters" in ts:
+            hyperparameters = {}
+            for i in ts["HyperParameters"]:
+                hyperparameters.update(
+                    {
+                        i["Name"]: i["Value"],
+                    }
+                )
+        else:
+            hyperparameters = None
+
+        base_job_name = f"{self.model_name}_training" if self.model_name else None
+
+        if "Scheduler" in ts and "MaxRunningTimeInSeconds" in ts["Scheduler"]:
+            max_run_time = ts["Scheduler"]["MaxRunningTimeInSeconds"]
+        else:
+            max_run_time = None
+
+        if "ComputeResource" in ts:
+            if (
+                "EcsSpec" in ts["ComputeResource"]
+                and "EcsCount" in ts["ComputeResource"]
+            ):
+                instance_type = ts["ComputeResource"]["EcsSpec"]
+                instance_count = ts["ComputeResource"]["EcsCount"]
+            elif (
+                "EcsSpec" not in ts["ComputeResource"]
+                and "EcsCount" not in ts["ComputeResource"]
+            ):
+                instance_type = None
+                instance_count = None
+            else:
+                instance_type = None
+                instance_count = None
+                logger.warning(
+                    "Only one of EcsCount and EcsType is provided. Please check the"
+                    " AlgorithmSpec."
+                )
+
+        return AlgorithmEstimator(
+            algorithm_name=algorithm_name,
+            algorithm_version=algorithm_version,
+            algorithm_provider=algorithm_provider,
+            algorithm_spec=algorithm_spec,
+            hyperparameters=hyperparameters,
+            base_job_name=base_job_name,
+            max_run_time=max_run_time,
+            instance_type=instance_type,
+            instance_count=instance_count,
+        )
+
+    def get_estimator_inputs(self) -> Dict[str, str]:
+        """Get the AlgorithmEstimator's default input channels
+
+        Get the AlgorithmEstimator's default input channels from RegisteredModel's
+        training_spec.
+
+        Returns:
+            dict[str, str]: A dict of input channels.
+        """
+        if not self.training_spec:
+            raise ValueError(
+                "The provided registered model does not contain training spec."
+            )
+        ts = self.training_spec
+        if "AlgorithmSpec" not in ts and "AlgorithmName" not in ts:
+            raise ValueError(
+                "The provided registered model's training spec does not contain any"
+                " algorithms."
+            )
+
+        input_channels = {}
+        if "InputChannels" in ts:
+            for i in ts["InputChannels"]:
+                input_channels.update(
+                    {
+                        i["Name"]: i["InputUri"],
+                    }
+                )
+        return input_channels
```

## pai/serializers.py

```diff
@@ -4,15 +4,14 @@
 from abc import ABC, abstractmethod
 from typing import Any, Dict, List, Optional, Tuple, Union
 from urllib.error import HTTPError
 
 import backoff
 import numpy
 import numpy as np
-import pandas as pd
 import six
 from eas_prediction import pytorch_predict_pb2 as pt_pb
 from eas_prediction import tf_request_pb2 as tf_pb
 
 from pai.session import Session, config_default_session
 
 logger = logging.getLogger(__name__)
@@ -23,14 +22,32 @@
         from PIL import Image
 
         return isinstance(data, Image.Image)
     except ImportError:
         return False
 
 
+def _is_numpy_ndarray(data) -> bool:
+    try:
+        import numpy
+
+        return isinstance(data, numpy.ndarray)
+    except ImportError:
+        return False
+
+
+def _is_pandas_dataframe(data) -> bool:
+    try:
+        import pandas
+
+        return isinstance(data, pandas.DataFrame)
+    except ImportError:
+        return False
+
+
 class TensorFlowIOSpec(object):
     def __init__(self, name: str, shape: Tuple, data_type: tf_pb.ArrayDataType):
         """A class represents TensorFlow inputs/outputs spec.
 
         Args:
             name (str): Name for the spec.
             shape (Tuple): The shape of the input/output value.
@@ -88,17 +105,18 @@
 class JsonSerializer(SerializerBase):
     """A Serializer object that serialize input data into JSON format and deserialize
     JSON formatted data into python object."""
 
     def serialize(self, data) -> bytes:
         if isinstance(data, six.string_types):
             return data
-        if isinstance(data, pd.DataFrame):
+
+        if _is_pandas_dataframe(data):
             data = data.to_numpy().tolist()
-        elif isinstance(data, np.ndarray):
+        elif _is_numpy_ndarray(data):
             data = data.tolist()
         return json.dumps(data).encode()
 
     def deserialize(self, data):
         return json.loads(data)
 
 
@@ -121,15 +139,15 @@
 
     def __init__(
         self,
     ):
         """TensorflowSerializer initializer."""
 
         self._input_specs = []
-        self._output_fileter = []
+        self._output_filter = []
         self._signature_name = None
         super(TensorFlowSerializer, self).__init__()
 
     @config_default_session
     def inspect_from_service(
         self, service_name: str, *, session: Optional[Session] = None
     ):
@@ -221,16 +239,16 @@
 
     def serialize(self, data: Union[Dict[str, Any], tf_pb.PredictRequest]) -> bytes:
 
         if isinstance(data, tf_pb.PredictRequest):
             return data.SerializeToString()
 
         request = tf_pb.PredictRequest()
-        if self._output_fileter:
-            for output_name in self._output_fileter:
+        if self._output_filter:
+            for output_name in self._output_filter:
                 request.output_filter.append(output_name)
 
         if not isinstance(data, dict):
             if not self._input_specs or len(self._input_specs) > 1:
                 raise ValueError(
                     "TensorFlowSerializer accepts a dictionary as input data, "
                     "with each input value having a name."
@@ -325,16 +343,16 @@
             output_specs.append(output_spec)
 
         if not self._signature_name:
             self._signature_name = signature_def_key
 
         if not self._input_specs:
             self._input_specs = input_specs
-        if not self._output_fileter:
-            self._output_fileter = [spec.name for spec in output_specs]
+        if not self._output_filter:
+            self._output_filter = [spec.name for spec in output_specs]
 
     def deserialize(self, data: bytes):
         response = tf_pb.PredictResponse()
         response.ParseFromString(data)
         output_names = response.outputs.keys()
         results = {}
         for name in output_names:
```

## pai/api/api_container.py

```diff
@@ -7,14 +7,15 @@
 from pai.api.dataset import DatasetAPI
 from pai.api.image import ImageAPI
 from pai.api.job import JobAPI
 from pai.api.model import ModelAPI
 from pai.api.pipeline import PipelineAPI
 from pai.api.pipeline_run import PipelineRunAPI
 from pai.api.service import ServiceAPI
+from pai.api.tensorboard import TensorBoardAPI
 from pai.api.training_job import TrainingJobAPI
 from pai.api.workspace import WorkspaceAPI
 
 
 class ResourceAPIsContainerMixin(object):
     """ResourceAPIsContainerMixin provides Resource Operation APIs."""
 
@@ -174,23 +175,34 @@
         self.api_container[PAIRestResourceTypes.PipelineRun] = PipelineRunAPI(
             workspace_id=self._workspace_id,
             acs_client=self._acs_paiflow_client,
             header=self.header,
             runtime=self.runtime,
         )
 
+        self.api_container[PAIRestResourceTypes.TensorBoard] = TensorBoardAPI(
+            workspace_id=self._workspace_id,
+            acs_client=self._acs_dlc_client,
+            header=self.header,
+            runtime=self.runtime,
+        )
+
     def get_api_by_resource(self, resource_type):
         return self.api_container[resource_type]
 
     @property
     def job_api(self) -> JobAPI:
         """Returns JobAPI for job operation."""
         return self.api_container[PAIRestResourceTypes.DlcJob]
 
     @property
+    def tensorboard_api(self) -> TensorBoardAPI:
+        return self.api_container[PAIRestResourceTypes.TensorBoard]
+
+    @property
     def code_source_api(self) -> CodeSourceAPI:
         """Return CodeSource API for code_source operation"""
         return self.api_container[PAIRestResourceTypes.CodeSource]
 
     @property
     def dataset_api(self) -> DatasetAPI:
         """Return Dataset API for dataset operation"""
```

## pai/api/base.py

```diff
@@ -1,15 +1,17 @@
 import logging
 from abc import ABCMeta
 from typing import Any, Dict, List, Optional, Union
 
+import backoff
 import six
 from alibabacloud_tea_openapi.client import Client
 from alibabacloud_tea_util.models import RuntimeOptions
 from six import with_metaclass
+from Tea.exceptions import TeaException
 from Tea.model import TeaModel
 
 logger = logging.getLogger(__name__)
 
 
 class PAIServiceName(object):
     PAI_DLC = "PAI_DLC"
@@ -29,14 +31,18 @@
     Service = "Service"
     Model = "Model"
     Workspace = "Workspace"
     Algorithm = "Algorithm"
     TrainingJob = "TrainingJob"
     Pipeline = "Pipeline"
     PipelineRun = "PipelineRun"
+    TensorBoard = "TensorBoard"
+
+
+RETRYABLE_REQUEST_METHOD_PREFIX = ["list_", "get_", "describe_"]
 
 
 class ResourceAPI(with_metaclass(ABCMeta, object)):
     """Class that provide APIs to operate the resource."""
 
     def __init__(
         self,
@@ -59,22 +65,35 @@
         self.header = header
         self.runtime = runtime
 
     def _make_extra_request_options(self):
         """Returns headers and runtime for client."""
         return self.header or dict(), self.runtime or RuntimeOptions()
 
-    def _do_request(self, method_, *args, **kwargs):
+    def _do_request(self, method_: str, *args, **kwargs):
         headers, runtime = self._make_extra_request_options()
         if "headers" not in kwargs:
             kwargs["headers"] = headers
         if "runtime" not in kwargs:
             kwargs["runtime"] = runtime
-        resp = getattr(self.acs_client, method_)(*args, **kwargs)
-        return resp.body
+        request_method = getattr(self.acs_client, method_)
+
+        if self.is_retryable_method(method_):
+            request_method = backoff.on_exception(
+                backoff.expo, exception=TeaException, max_tries=3
+            )(request_method)
+
+        return request_method(*args, **kwargs).body
+
+    @classmethod
+    def is_retryable_method(cls, method_: str):
+        for prefix in RETRYABLE_REQUEST_METHOD_PREFIX:
+            if method_.startswith(prefix):
+                return True
+        return False
 
     def get_api_object_by_resource_id(self, resource_id):
         raise NotImplementedError
 
     def refresh_entity(self, id_, entity):
         """Refresh entity using API object from service."""
         if not isinstance(id_, six.string_types) and not isinstance(
@@ -126,35 +145,28 @@
     def __init__(self, workspace_id, acs_client, **kwargs):
         super(WorkspaceScopedResourceAPI, self).__init__(
             acs_client=acs_client, **kwargs
         )
         self.workspace_id = workspace_id
 
     def _do_request(self, method_, **kwargs):
-        headers, runtime = self._make_extra_request_options()
-        if "headers" not in kwargs:
-            kwargs["headers"] = headers
-        if "runtime" not in kwargs:
-            kwargs["runtime"] = runtime
         request = kwargs.get("request")
 
         # Automatically configure the workspace ID for the request
         if request and hasattr(request, "workspace_id"):
             if request.workspace_id is None:
                 request.workspace_id = self.workspace_id
             elif (
                 request.workspace_id == self.workspace_id_none_placeholder
                 or not request.workspace_id
             ):
                 # request.workspace_id is 0 or request.workspace_id is empty string,
                 # we do not inject workspace_id of the scope.
                 request.workspace_id = None
-
-        resp = getattr(self.acs_client, method_)(**kwargs)
-        return resp.body
+        return super(WorkspaceScopedResourceAPI, self)._do_request(method_, **kwargs)
 
 
 class PaginatedResult(object):
     """A class represent response of a pagination call to PAI service."""
 
     items: List[Union[Dict[str, Any], str]] = None
     total_count: int = None
```

## pai/api/model.py

```diff
@@ -1,8 +1,9 @@
 import typing
+from typing import Any, Dict
 
 from pai.api.base import PaginatedResult, WorkspaceScopedResourceAPI
 from pai.libs.alibabacloud_aiworkspace20210204.models import (
     CreateModelRequest,
     CreateModelVersionRequest,
     Label,
     ListModelsRequest,
@@ -20,176 +21,220 @@
 
     _create_model_method = "create_model_with_options"
     _list_model_method = "list_models_with_options"
     _get_model_method = "get_model_with_options"
     _delete_model_method = "delete_model_with_options"
 
     _create_model_version_method = "create_model_version_with_options"
+    _list_model_version_method = "list_model_versions_with_options"
     _get_model_version_method = "get_model_version_with_options"
     _update_model_version_method = "update_model_version_with_options"
-    _list_model_version_method = "list_model_versions_with_options"
     _delete_model_version_method = "delete_model_version_with_options"
 
-    def get_api_object_by_resource_id(self, model_id):
-        resp = self._do_request(self._get_model_method, model_id=model_id)
-        return resp.to_map()
-
-    def get_version_api_object(self, model_id, version):
-        resp = self._do_request(
-            self._get_model_version_method, model_id=model_id, version_name=version
-        )
-        return resp.to_map()
-
-    def create_from_api_object(self, api_object):
-        request = CreateModelRequest().from_map(api_object)
-
-        resp = self._do_request(self._create_model_method, request=request)
-        return resp.model_id
-
-    def create(self, name, accessibility=None, labels=None, description=None):
-
+    def create(
+        self,
+        accessibility: str = None,
+        domain: str = None,
+        labels: Dict[str, str] = None,
+        model_description: str = None,
+        model_doc: str = None,
+        model_name: str = None,
+        origin: str = None,
+        task: str = None,
+        workspace_id: str = None,
+    ) -> str:
         labels = [Label(key=k, value=v) for k, v in labels.items()] if labels else []
 
         request = CreateModelRequest(
-            model_name=name,
             accessibility=accessibility,
+            domain=domain,
             labels=labels,
-            model_description=description,
+            model_description=model_description,
+            model_doc=model_doc,
+            model_name=model_name,
+            origin=origin,
+            task=task,
+            workspace_id=workspace_id,
         )
 
         resp = self._do_request(self._create_model_method, request=request)
         return resp.model_id
 
-    def get(self, model_id):
+    def list(
+        self,
+        domain: str = None,
+        label: str = None,
+        label_string: str = None,
+        labels: str = None,
+        model_name: str = None,
+        order: str = None,
+        origin: str = None,
+        page_number: int = None,
+        page_size: int = None,
+        provider: str = None,
+        query: str = None,
+        sort_by: str = None,
+        task: str = None,
+        workspace_id: str = None,
+    ) -> PaginatedResult:
+        request = ListModelsRequest(
+            domain=domain,
+            label=label,
+            label_string=label_string,
+            labels=labels,
+            model_name=model_name,
+            order=order,
+            origin=origin,
+            page_number=page_number,
+            page_size=page_size,
+            provider=provider,
+            query=query,
+            sort_by=sort_by,
+            task=task,
+            workspace_id=workspace_id,
+        )
+
+        resp: ListModelsResponseBody = self._do_request(
+            self._list_model_method, request=request
+        )
+        return self.make_paginated_result(resp)
+
+    def get(self, model_id: str):
         resp = self._do_request(method_=self._get_model_method, model_id=model_id)
         return resp.to_map()
 
-    def delete(self, model_id):
+    def delete(self, model_id: str):
         self._do_request(method_=self._delete_model_method, model_id=model_id)
 
     def create_version(
         self,
-        model_id,
-        uri,
-        version=None,
-        model_format=None,
-        framework=None,
-        labels=None,
-        inference_spec=None,
-        description=None,
+        model_id: str,
+        approval_status: str = None,
+        format_type: str = None,
+        framework_type: str = None,
+        inference_spec: Dict[str, Any] = None,
+        labels: Dict[str, str] = None,
+        metrics: Dict[str, Any] = None,
+        options: str = None,
+        source_id: str = None,
+        source_type: str = None,
+        training_spec: Dict[str, Any] = None,
+        uri: str = None,
+        version_description: str = None,
+        version_name: str = None,
     ):
         """Create a ModeVersion resource."""
-
         labels = [Label(key=k, value=v) for k, v in labels.items()] if labels else []
 
         request = CreateModelVersionRequest(
-            version_name=version,
-            format_type=model_format,
-            framework_type=framework,
+            approval_status=approval_status,
+            format_type=format_type,
+            framework_type=framework_type,
             inference_spec=inference_spec,
             labels=labels,
+            metrics=metrics,
+            options=options,
+            source_id=source_id,
+            source_type=source_type,
+            training_spec=training_spec,
             uri=uri,
-            version_description=description,
-            # options=None,
-            # source_id=None,
-            # source_type=None,
+            version_description=version_description,
+            version_name=version_name,
         )
 
         response = self._do_request(
             self._create_model_version_method, model_id=model_id, request=request
         )
 
         version_name = response.to_map()["VersionName"]
         return version_name
 
-    def get_model_version_api_object(self, model_id, version):
-        resp = self._do_request(
-            self._get_model_version_method, model_id=model_id, version_name=version
-        )
-        obj = resp.to_map()
-        obj.update({"ModelId": model_id})
-        return obj
-
-    def get_version(self, model_id, version):
-        return self.get_model_version_api_object(model_id=model_id, version=version)
-
-    def refresh_version(self, model_version):
-        api_obj = self.get_model_version_api_object(
-            model_id=model_version.model_id, version=model_version.version
-        )
-        model_version.patch_from_api_object(api_obj)
-
-    def list(
-        self, name=None, order=None, page_number=None, page_size=None, sort_by=None
-    ) -> PaginatedResult:
-
-        request = ListModelsRequest(
-            model_name=name,
-            order=order,
-            page_number=page_number,
-            page_size=page_size,
-            sort_by=sort_by,
-            # labels=None,
-            # label_string=None,
-        )
-
-        resp: ListModelsResponseBody = self._do_request(
-            self._list_model_method, request=request
-        )
-        return self.make_paginated_result(resp)
-
     def list_versions(
         self,
         model_id,
-        framework=None,
-        model_format=None,
-        page_number=None,
-        page_size=None,
-        order=None,
-        sort_by=None,
+        approval_status: str = None,
+        format_type: str = None,
+        framework_type: str = None,
+        label: str = None,
+        label_string: str = None,
+        labels: str = None,
+        order: str = None,
+        page_number: int = None,
+        page_size: int = None,
+        sort_by: str = None,
+        source_id: str = None,
+        source_type: str = None,
+        version_name: str = None,
     ) -> PaginatedResult:
-
         request = ListModelVersionsRequest(
-            format_type=model_format,
-            framework_type=framework,
-            # label_string=None,
-            # labels=None,
+            approval_status=approval_status,
+            format_type=format_type,
+            framework_type=framework_type,
+            label=label,
+            label_string=label_string,
+            labels=labels,
             order=order,
-            sort_by=sort_by,
-            # version_name=None,
-            # source_id=None,
-            # source_type=None,
             page_number=page_number,
             page_size=page_size,
+            sort_by=sort_by,
+            source_id=source_id,
+            source_type=source_type,
+            version_name=version_name,
         )
 
         resp: ListModelVersionsResponseBody = self._do_request(
             self._list_model_version_method, model_id=model_id, request=request
         )
 
         data = resp.to_map()
-
         for v in data["Versions"]:
             v.update(
                 {
                     "ModelId": model_id,
                 }
             )
         return self.make_paginated_result(data)
 
-    def update_version(self, model_id, version, inference_spec=None, description=None):
+    def get_version(self, model_id: str, version: str):
+        resp = self._do_request(
+            self._get_model_version_method, model_id=model_id, version_name=version
+        )
+        obj = resp.to_map()
+        obj.update({"ModelId": model_id})
+        return obj
+
+    def update_version(
+        self,
+        model_id: str,
+        version: str,
+        approval_status: str = None,
+        inference_spec: Dict[str, Any] = None,
+        metrics: Dict[str, Any] = None,
+        options: str = None,
+        source_id: str = None,
+        source_type: str = None,
+        training_spec: Dict[str, Any] = None,
+        version_description: str = None,
+    ):
         request = UpdateModelVersionRequest(
-            inference_spec=inference_spec, version_description=description
+            approval_status=approval_status,
+            inference_spec=inference_spec,
+            metrics=metrics,
+            options=options,
+            source_id=source_id,
+            source_type=source_type,
+            training_spec=training_spec,
+            version_description=version_description,
         )
         self._do_request(
             self._update_model_version_method,
             model_id=model_id,
             version_name=version,
             request=request,
         )
 
-    def delete_version(self, model_id, version):
+    def delete_version(self, model_id: str, version: str):
         self._do_request(
             self._delete_model_version_method,
             model_id=model_id,
             version_name=version,
         )
```

## pai/api/training_job.py

```diff
@@ -1,9 +1,12 @@
 from typing import Any, Dict, List, Optional
 
+import backoff
+from Tea.exceptions import TeaException
+
 from pai.api.base import PaginatedResult, WorkspaceScopedResourceAPI
 from pai.libs.alibabacloud_paistudio20220112.models import (
     AlgorithmSpec,
     CreateTrainingJobRequest,
     CreateTrainingJobRequestComputeResource,
     CreateTrainingJobRequestHyperParameters,
     CreateTrainingJobRequestInputChannels,
```

## pai/common/docker_utils.py

```diff
@@ -1,16 +1,14 @@
 import io
 import logging
 import subprocess
 import time
 from random import randint
 from typing import Any, Dict, List, Optional, Union
 
-import docker
-
 logger = logging.getLogger(__name__)
 
 
 def _run_command(command: List[str], input: Optional[str] = None):
     with subprocess.Popen(
         command,
         stdin=subprocess.PIPE,
@@ -32,17 +30,15 @@
 class ContainerRun(object):
     """A class represent a container run in local."""
 
     CONTAINER_STATUS_RUNNING = "running"
     CONTAINER_STATUS_EXITED = "exited"
     CONTAINER_STATUS_PAUSED = "paused"
 
-    def __init__(
-        self, container: docker.models.containers.Container, port: Optional[int] = None
-    ):
+    def __init__(self, container, port: Optional[int] = None):
         """Initialize a container run.
 
         Args:
             container: A docker container object.
             port (int): The host port that container is exposed to.
 
         """
@@ -94,22 +90,26 @@
             self.container.start()
 
     def delete(self):
         if self.is_running():
             self.container.stop()
         self.container.remove()
 
-    def watch(self):
+    def watch(self, show_logs: bool = True):
         """Watch container log and wait for container to exit."""
-        log_iter = self.container.logs(
-            stream=True,
-            follow=True,
-        )
-        for log in log_iter:
-            print(log.decode())
+        if not show_logs:
+            self.container.wait()
+        else:
+            log_iter = self.container.logs(
+                stream=True,
+                follow=True,
+            )
+            for log in log_iter:
+                print(log.decode())
+
         self.container.reload()
         exit_code = self.container.attrs["State"]["ExitCode"]
         if exit_code != 0:
             raise RuntimeError(
                 "Container run exited failed: exit_code={}".format(exit_code)
             )
 
@@ -151,14 +151,19 @@
              Available capabilities for the NVIDIA driver can be found in
             https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html#driver-capabilities.
 
     Returns:
         ContainerRun: A ContainerRun object.
 
     """
+    try:
+        import docker
+    except ImportError:
+        raise ImportError("Please install docker first: pip install docker")
+
     client = docker.from_env()
     # use a random host port.
     host_port = randint(49152, 65535)
 
     if gpu_count or gpu_device_ids or gpu_capabilities:
         if not gpu_capabilities:
             gpu_capabilities = [["compute", "utility"]]
```

## pai/common/utils.py

```diff
@@ -2,33 +2,20 @@
 
 import random
 import re
 import string
 import sys
 from typing import Callable, Dict, Optional, Union
 
-import six
-
-from .. import __version__
-from .consts import INSTANCE_TYPE_LOCAL, INSTANCE_TYPE_LOCAL_GPU
+from pai import __version__
+from pai.common.consts import INSTANCE_TYPE_LOCAL, INSTANCE_TYPE_LOCAL_GPU
 
 DEFAULT_PLAIN_TEXT_ALLOW_CHARACTERS = string.ascii_letters + string.digits + "_"
 
 
-def iter_with_limit(iterator, limit):
-    if not isinstance(limit, six.integer_types) or limit <= 0:
-        raise ValueError("'limit' should be positive integer")
-    idx = 0
-    for item in iterator:
-        yield item
-        idx += 1
-        if idx >= limit:
-            return
-
-
 def is_iterable(arg):
     try:
         _ = iter(arg)
         return True
     except TypeError:
         return False
```

## pai/huggingface/model.py

```diff
@@ -137,15 +137,15 @@
 
         .. tip::
 
             You can find additional parameters for initializing this class at
             :class:`~pai.model.ModelBase`.
         """
         self._validate_args(
-            image_uri=image_uri, transformers_verison=transformers_version
+            image_uri=image_uri, transformers_version=transformers_version
         )
 
         self.model_data = model_data
         self.image_uri = image_uri
         self.transformers_version = transformers_version
         self.command = command
         self.source_dir = source_dir
@@ -161,17 +161,17 @@
         super(HuggingFaceModel, self).__init__(
             model_data=self.model_data,
             inference_spec=inference_spec,
             session=self.session,
             **kwargs,
         )
 
-    def _validate_args(self, image_uri: str, transformers_verison: str) -> None:
+    def _validate_args(self, image_uri: str, transformers_version: str) -> None:
         """Check if image_uri or transformers_version arguments are specified."""
-        if not image_uri and not transformers_verison:
+        if not image_uri and not transformers_version:
             raise ValueError(
                 "transformers_version, and image_uri are both None. "
                 "Specify either transformers_version or image_uri."
             )
 
     def serving_image_uri(self, instance_type: str) -> str:
         """Return the Docker image to use for serving.
```

## pai/pipeline/core.py

```diff
@@ -226,15 +226,15 @@
     def _build_outputs(cls, outputs):
         outputs = outputs or []
         if isinstance(outputs, dict):
             items = outputs.items()
         elif isinstance(outputs, (list, OutputsSpec)):
             items = [(item.name, item) for item in outputs]
         else:
-            raise ValueError("Require list or dict, unexpect type:%s" % type(outputs))
+            raise ValueError("Require list or dict, unexpected type:%s" % type(outputs))
 
         results = []
         for name, item in items:
             if isinstance(item, PipelineArtifact):
                 results.append(
                     PipelineArtifact(
                         repeated=item.repeated,
@@ -344,15 +344,21 @@
 
     def validate_step_name(self, name):
         if name in self.steps:
             raise ValueError("Pipeline step name conflict: %s" % name)
         return name
 
     def dot(self):
-        from graphviz import Digraph
+        try:
+            from graphviz import Digraph
+        except ImportError:
+            raise ImportError(
+                "Unable to display pipeline, install graphviz first: "
+                "pip install graphviz"
+            )
 
         graph = Digraph()
         for step in self.steps:
             graph.node(step.name)
             for head in step.depends:
                 graph.edge(head.name, step.name)
         return graph
```

## pai/pipeline/types/artifact.py

```diff
@@ -548,15 +548,15 @@
     def from_resource_url(cls, resource_url):
         """Parse MaxCompute(ODPS) resource in url schema and returns artifact value and metadata.
 
         Args:
             resource_url: An ODPS(MaxCompute) table, tablePartition, offline-model or volume in url schema.
 
         Returns:
-            tuple: A tuple of  MaxCompute artifact value and aritfact metadata.
+            tuple: A tuple of  MaxCompute artifact value and artifact metadata.
 
         """
         matches = cls.MaxComputeResourceUrlPattern.match(resource_url)
         if not matches:
             raise ValueError("Not support MaxCompute resource url format.")
         resource_type = matches.group("resource_type")
         project = matches.group("project")
```

## pai/schema/training_job_schema.py

```diff
@@ -79,14 +79,15 @@
     create_time = fields.DateTime(load_only=True)
     modified_time = fields.DateTime(load_only=True)
     reason_code = fields.Str()
     reason_message = fields.Str()
     status = fields.Str()
     status_transitions = fields.List(fields.Dict)
     training_job_id = fields.Str()
+    training_job_url = fields.Str()
 
     @post_load
     def _make(self, data, **kwargs):
         from pai.estimator import _TrainingJob
 
         data["instance_count"] = data.get("compute_resource", {}).get("EcsCount")
         data["instance_type"] = data.get("compute_resource", {}).get("EcsType")
```

## Comparing `alipai-0.4.1.dist-info/LICENSE.txt` & `alipai-0.4.2.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `alipai-0.4.1.dist-info/METADATA` & `alipai-0.4.2.dist-info/METADATA`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,36 @@
 Metadata-Version: 2.1
 Name: alipai
-Version: 0.4.1
+Version: 0.4.2
 Summary: Alibaba Cloud PAI Python SDK
 Home-page: https://www.aliyun.com/product/bigdata/product/learn
 Author: Alibaba PAI team
 License: Apache License 2.0
 Keywords: ML Alibaba Cloud PAI Training Inference Pipeline
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 Requires-Dist: aliyun-python-sdk-core (>=2.13.25)
 Requires-Dist: alibabacloud-sts20150401
-Requires-Dist: graphviz
 Requires-Dist: numpy (>=1.16.0)
 Requires-Dist: oss2 (>=2.8.0)
 Requires-Dist: pyodps (>=0.11.0)
 Requires-Dist: pyyaml (>=5.3.1)
 Requires-Dist: six (>=1.15.0)
-Requires-Dist: docker (>=4.4.0)
 Requires-Dist: marshmallow
 Requires-Dist: marshmallow-oneofschema (>=3.0.1)
 Requires-Dist: eas-prediction (>=0.20)
 Requires-Dist: alibabacloud-tea-util (!=0.3.9,<1.0.0,>=0.3.6)
 Requires-Dist: alibabacloud-tea-openapi (<1.0.0,>=0.3.3)
 Requires-Dist: alibabacloud-openapi-util (<1.0.0,>=0.1.6)
 Requires-Dist: alibabacloud-endpoint-util (<1.0.0,>=0.0.3)
-Requires-Dist: pandas
 Requires-Dist: addict
 Requires-Dist: backoff (<2.0.0)
 Requires-Dist: semantic-version
+Requires-Dist: requests
 Requires-Dist: tqdm
 Requires-Dist: prompt-toolkit
 Requires-Dist: pyOpenSSL (>=23.0.0)
 Requires-Dist: aiohttp
+Requires-Dist: importlib-metadata ; python_version < "3.8"
 
 # Alibaba PAI Python SDK
```

## Comparing `alipai-0.4.1.dist-info/RECORD` & `alipai-0.4.2.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,43 @@
-pai/VERSION,sha256=9iGEzuh4fy9pQcggQaVyXU7cmqKT6-Xb9mRAboLsH-E,6
-pai/__init__.py,sha256=8EbzTuHLzyuU4ZWOrLhK2p8fRJk7cWpSK9htMS4fLRA,290
-pai/estimator.py,sha256=L6v4ZO3QfiJEwbLwS2qfZXvM60a4b-vjwRmJycnA2xI,40574
-pai/exception.py,sha256=xfvKD1e9fel4t3MXsOGx4Qsvc-IVvMxly74rXCQlLeE,489
+pai/__init__.py,sha256=aEVNhcNeawDL-vVjPuJ-6wMeA29gq9cJMC15MQc3UcY,368
+pai/estimator.py,sha256=bbWHRueqsjey1n9iaHSfe7D3H8BHnTUgUe0GZ7iaAsk,70483
+pai/exception.py,sha256=4sQD9Uy91TdQ8bcvw0IkEURzunUvocM4nb8j41am7vo,729
 pai/image.py,sha256=EEqPdQ7wPZTAh8wE3N30dejyZd4korrqSI4UTrqtOU4,12163
-pai/model.py,sha256=BrPX78RJZq8-l-Cn8XhK1TJp1E7_LzkVL2tMS87IKD0,40522
+pai/model.py,sha256=hwqgKa33wJP8tSlI2II9ql1UzW-FIys2tN28dLbiD-4,64524
 pai/predictor.py,sha256=3-dnXqvq9nPDb5VGeDKmL7BRu9fFMQMuYDlPe_4jjDQ,46831
-pai/serializers.py,sha256=cegGv0BLwTrg3ubkEYRSGHq2xnBjaXNawKbSRInSFPk,20082
+pai/serializers.py,sha256=lyP8imTaHyNELOPyDlOQf7H9FhGl-9AVZWn-HaYilrc,20380
 pai/session.py,sha256=lveC30bset962AbW7Dbv62NvW-7az-UXydpzUGNby2o,14538
+pai/tensorboard.py,sha256=XiYMfm27ayVyA4J8zbZyN0zHslcKZwlOR7R74vLpt7U,6141
 pai/api/__init__.py,sha256=YV8TG1oW4YTohSL8XonH-ARm46SoGBV-wjOrW-wsi8I,39
 pai/api/algorithm.py,sha256=xS01gHJteqOHAg1CaJe6OHQGV24qAy9BTSCVVOAR6BE,4249
-pai/api/api_container.py,sha256=Fd9uJqOd1A0Mu1dtTTVlLzbpMkfryeSdSRGsU2ahFxk,7935
-pai/api/base.py,sha256=IllrMZIbqS1PyO50fg9p3CNirzPVRkTmm-eQ1qdDnSw,5439
+pai/api/api_container.py,sha256=KVJHQw-zSyMJfx8z86RcHh4ADL2-4lGCtyvXtI-943A,8360
+pai/api/base.py,sha256=ovyH53bcSus8XCwM2KfsGB5bXOB0s1P86qGin5BGMp0,5813
 pai/api/client_factory.py,sha256=rn5jcgWYrZh4X7RXxFn2ampzW2XPxmtfF1Bp29EvsT8,4010
 pai/api/code_source.py,sha256=4DN-EDWwEvKazdy4ejSgugVEqMLDWUfgw9zAYdxIVfg,2945
 pai/api/dataset.py,sha256=pnooRnQ-N3Ln4DJ7RVU94PYb6T9UwpPJqBFijZj17_0,3755
 pai/api/entity_base.py,sha256=SJx-05R657MJSrEf7px0czLMdoyb4ao4Wfi95CcXXgk,1726
 pai/api/image.py,sha256=7shc_4hZdGrxqxAjM-4KO9VwASdvk-MqZ-wlkLAxjyM,2665
 pai/api/job.py,sha256=eLLlTvyeK-LVbRK666djQUI2XC3PqK1sG6wN8JAPiGE,5232
-pai/api/model.py,sha256=mvEOGGMx8AE29QSLFsmAZoGY4xh1f9vc5Z3NmXWzB0E,6090
+pai/api/model.py,sha256=2BymhdTp-phD-_2Dizy-d-c2SULFyvHKQLyKlL64omU,7497
 pai/api/pipeline.py,sha256=_mvo8a3qkE9BI6B0AqeaZJ4VwZzEYe7AAtZlcZKQ_bk,3426
 pai/api/pipeline_run.py,sha256=UVKKt1AUF8mAvQHGqTtbLJ-ZYBRc_eOQ2CgQEoPF_Ec,5314
 pai/api/service.py,sha256=n6-R3IEWTg3YAzSoh82mv6NXQQbl-l3hAofjmzO9HgE,5421
-pai/api/training_job.py,sha256=koOYf8GuwNyH26Tvd4UgzNAayXLTZHGvIyi-CTBtdIs,5410
+pai/api/tensorboard.py,sha256=Q0RYXTBCZsi0GFB0QboEjmYK7qDAzt4SlViEyQH3yoo,3374
+pai/api/training_job.py,sha256=VEpmDTROGg8h8sZt7b9l16V9M10rR5toGojBxq_3kN4,5466
 pai/api/workspace.py,sha256=6CWRrbRQGHEn7Tzij3eDrpycHVQwUOqV3uwR9dXVEwg,8340
 pai/common/__init__.py,sha256=vwTRpeN4lv0dVtk__5azpaR9RH4H1tNSl_O_5_gpGME,67
 pai/common/consts.py,sha256=oTBSJAoSsIhoAxepB_7rxUyJMqbd9TbDH2fJvGovuk0,1565
-pai/common/docker_utils.py,sha256=Lcw9h06vcgLUh-1vlZmxcM1CuHwyDFpFTnnCDaYphWg,6438
+pai/common/docker_utils.py,sha256=7SKAw41N6sTfLwlBdPtLUXx-lVP8Y2HBBr7xTpe3Edg,6629
 pai/common/git_utils.py,sha256=k-q9Ef8vzhJnup3jSPgUj1BtvJx7INxBDGL0r89wtVo,12168
 pai/common/oss_utils.py,sha256=JLWcEKC9hJxkyKHul4YkQpAsjbcvZKQprKY6jByTkKY,14998
-pai/common/utils.py,sha256=VnnUExzq_7KBwK7cV40t_WZpaDLzW2AtZYLfyppX7D4,3406
+pai/common/utils.py,sha256=oPmQvKSOLC8-EmE_mtQfmcf1sXr1O1o9MLjwnnwC8Yc,3121
 pai/common/yaml_utils.py,sha256=GAupYkXylpc0COo01X_bEQ_NTvUELwT_wWjIJ6SWoUM,710
 pai/huggingface/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/huggingface/estimator.py,sha256=OjY0LK5Y1ltNgUhjp97IabmjirSVIBSqsuv9iONbkmw,10037
-pai/huggingface/model.py,sha256=L_EHl1t1Vip83kMNFYZOxfnCS8Bki1JC8tya93RnSJk,12791
+pai/huggingface/model.py,sha256=agjkCtUBDog-YwKib04ifwkXU6L4pUBtQXH2wV8vZ_4,12791
 pai/libs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/libs/alibabacloud_aiworkspace20210204/__init__.py,sha256=eSyU24SgU3Uua45_Fv26TJ9RzfxBzBPCt7REcbfUSts,22
 pai/libs/alibabacloud_aiworkspace20210204/client.py,sha256=RqUMgEu7zo-pXPGrGhOJyV3ks5jVPjdNOKtUlP-PA0Q,278209
 pai/libs/alibabacloud_aiworkspace20210204/models.py,sha256=ObYQlVZxqboaU57UMG3Tbtcd1-SOB32aMy3E_sZzk8Y,400899
 pai/libs/alibabacloud_eas20210701/__init__.py,sha256=U8BEyItnCRffb5NQFYKwkJf1OI4ctsrASQHqqSyNC4k,21
 pai/libs/alibabacloud_eas20210701/client.py,sha256=eZ4YeSWhS1rPMI9o2nH1lt8RZIFHkWeOLcyOC74gjwk,188668
 pai/libs/alibabacloud_eas20210701/models.py,sha256=a_4GRIksK03ZrREBqw90Nu5dTyijXgidCb9dtF5QN5w,251200
@@ -51,32 +52,32 @@
 pai/libs/alibabacloud_paistudio20220112/models.py,sha256=-ajbmwg906SqGCLr-RY4drqt3VpDzYrlgXUiMD2GNDg,236081
 pai/modelscope/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/modelscope/estimator.py,sha256=gWy4jgLFGiBgW8fXdZrwjm9sv2JNJMbn2ltQF12Xulg,9985
 pai/modelscope/model.py,sha256=vkJPZHWywb07ZBuGi9N_0D1XfcMmfU_nhX2Vmm03coo,12704
 pai/pipeline/__init__.py,sha256=UPvgppQ80FlOSP0bhdNjlIwu4GotIYOcVtNYjrFZcNc,503
 pai/pipeline/artifact.py,sha256=BNBrJ7D-Mazy8nmjlQGLBmJ7f3h5g07_-ehzXiKpwF4,2463
 pai/pipeline/consts.py,sha256=PLEBK7ctL6tlELEOyBUPeb5g55uSCwWdIQDSr4RIg28,81
-pai/pipeline/core.py,sha256=IRmKUNMi_zwpUbThSf4_ZtjiRZxIw1LoWjCjXvlfQmI,13115
+pai/pipeline/core.py,sha256=bO8oh-HMRkeLNyt7zRjpmhZqx3liS1kshn9OAQLGELs,13317
 pai/pipeline/run.py,sha256=DFD5ITljfP3m6RdFMSGQttVykcHepykFW0_6jxjjeAs,15710
 pai/pipeline/step.py,sha256=9OSh8iIEfe6VefrKedXbuBjnGIKWrYZ96vPEk3K4Hss,12848
 pai/pipeline/component/__init__.py,sha256=tMs-kbh4_m39wv12oUSOFqurW1RwEKVwNeQXdJFaTpo,128
 pai/pipeline/component/_base.py,sha256=z-9u7MYx7sUs-8oRC3H7e5ZyDdgzHtNprEqSIHRhqQ8,9801
 pai/pipeline/component/_container.py,sha256=KQC143S50gNOHDvhA-qaeN1pYvv2AXPcuIMFgG8VUfI,2681
 pai/pipeline/component/_registered.py,sha256=EJufxjpoVsbFZ1eYnRFvSkxSJo4mf87WTjAlyYFttDQ,8522
 pai/pipeline/types/__init__.py,sha256=zhTcjc3SJ39QVt2qkIYOXQ98PObGdOLwhsP4rygnc28,667
-pai/pipeline/types/artifact.py,sha256=OUpKVCkarPg5dvyYRZ7KQM-sEk8NdEbQZINb-VCzm6M,25584
+pai/pipeline/types/artifact.py,sha256=O4IyIYz-2kAxDooNHZCuBHITmTPLOJ4iA9pdiuHwwKQ,25584
 pai/pipeline/types/parameter.py,sha256=amifhoPs8_Ww5ADJq7wD__Y7biw2Rh6OfUDimI_E4o4,9725
 pai/pipeline/types/spec.py,sha256=kO_6vZ0LJuYaSw_DRyjR9eOhmDgiIX6hfWLX9KRSKlo,6055
 pai/pipeline/types/variable.py,sha256=upm1YNss4kSiAX8QBLmjXgo5t249b-M4_knDpgL-JaI,4249
 pai/schema/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/schema/base.py,sha256=R7xg4mREKxPqKvYOasLXE77N1PQjKAIPLzL_bAtOkOs,2871
-pai/schema/training_job_schema.py,sha256=CEb2oq4lu5oubeQjY4zRTPTeAykb5ojCSGslzpmiHAQ,2857
+pai/schema/training_job_schema.py,sha256=dM1iHJX0F0ZhwQUQDvQkBU3o4k4e-L0x-P94zX5HOZQ,2893
 pai/toolkit/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/toolkit/config.py,sha256=ZIXiLiqOqPMWYxw6xJMrddsNnHxgU0RjqlxzNc7MueM,16185
 pai/toolkit/helper/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pai/toolkit/helper/consts.py,sha256=KqU5jABCeIYct6ETGMnOW7-pcYoWOW8YKKtw5KwTANA,2559
 pai/toolkit/helper/utils.py,sha256=Mjku3RNnaYUX4ICa30ap1OKJDnRGJIA4umpQDrdp6K0,10835
-alipai-0.4.1.dist-info/LICENSE.txt,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-alipai-0.4.1.dist-info/METADATA,sha256=vM-2wTdEn5H9Wu6Byb8GKtqjj1ehxMXVAJ8odp6klfU,1250
-alipai-0.4.1.dist-info/WHEEL,sha256=Z-nyYpwrcSqxfdux5Mbn_DQ525iP7J2DG3JgGvOYyTQ,110
-alipai-0.4.1.dist-info/top_level.txt,sha256=1jOjp5NlWBLogWe-8LLNotO51tqwsu2J38adsPBHuoc,4
-alipai-0.4.1.dist-info/RECORD,,
+alipai-0.4.2.dist-info/LICENSE.txt,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+alipai-0.4.2.dist-info/METADATA,sha256=DyyhWkqfFMjoF_bBHyaHP6B_Fdku8PMzqZffE6ffZjs,1255
+alipai-0.4.2.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+alipai-0.4.2.dist-info/top_level.txt,sha256=1jOjp5NlWBLogWe-8LLNotO51tqwsu2J38adsPBHuoc,4
+alipai-0.4.2.dist-info/RECORD,,
```

