# Comparing `tmp/flardl-0.0.6.tar.gz` & `tmp/flardl-0.0.7.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flardl-0.0.6.tar", max compression
+gzip compressed data, was "flardl-0.0.7.tar", max compression
```

## Comparing `flardl-0.0.6.tar` & `flardl-0.0.7.tar`

### file list

```diff
@@ -1,14 +1,14 @@
--rw-r--r--   0        0        0     1958 2023-07-26 20:59:49.908917 flardl-0.0.6/LICENSE
--rw-r--r--   0        0        0      243 2023-07-26 20:59:49.908917 flardl-0.0.6/LICENSE.logo.txt
--rw-r--r--   0        0        0     7841 2023-07-26 20:59:49.908917 flardl-0.0.6/README.md
--rw-r--r--   0        0        0     2780 2023-07-26 21:00:01.512897 flardl-0.0.6/pyproject.toml
--rw-r--r--   0        0        0      673 2023-07-26 20:59:49.912917 flardl-0.0.6/src/flardl/__init__.py
--rw-r--r--   0        0        0     3380 2023-07-26 20:59:49.912917 flardl-0.0.6/src/flardl/common.py
--rw-r--r--   0        0        0     1581 2023-07-26 20:59:49.916917 flardl-0.0.6/src/flardl/dict_to_indexed_list.py
--rwxr-xr-x   0        0        0     8243 2023-07-26 21:00:01.512897 flardl-0.0.6/src/flardl/downloader.py
--rwxr-xr-x   0        0        0     4225 2023-07-26 20:59:49.916917 flardl-0.0.6/src/flardl/instrumented_streams.py
--rwxr-xr-x   0        0        0     7663 2023-07-26 21:00:01.512897 flardl-0.0.6/src/flardl/multidispatcher.py
--rw-r--r--   0        0        0        0 2023-07-26 20:59:49.916917 flardl-0.0.6/src/flardl/py.typed
--rw-r--r--   0        0        0      651 2023-07-26 20:59:49.916917 flardl-0.0.6/src/flardl/server_defs.py
--rwxr-xr-x   0        0        0    11622 2023-07-26 20:59:49.916917 flardl-0.0.6/src/flardl/stream_stats.py
--rw-r--r--   0        0        0     9181 1970-01-01 00:00:00.000000 flardl-0.0.6/PKG-INFO
+-rw-r--r--   0        0        0     1958 2023-08-03 23:12:16.444260 flardl-0.0.7/LICENSE
+-rw-r--r--   0        0        0      243 2023-08-03 23:12:16.444260 flardl-0.0.7/LICENSE.logo.txt
+-rw-r--r--   0        0        0     8565 2023-08-03 23:12:16.444260 flardl-0.0.7/README.md
+-rw-r--r--   0        0        0     2922 2023-08-03 23:12:27.892380 flardl-0.0.7/pyproject.toml
+-rw-r--r--   0        0        0      673 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/__init__.py
+-rw-r--r--   0        0        0     3380 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/common.py
+-rw-r--r--   0        0        0     1581 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/dict_to_indexed_list.py
+-rwxr-xr-x   0        0        0     8241 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/downloader.py
+-rwxr-xr-x   0        0        0     4225 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/instrumented_streams.py
+-rwxr-xr-x   0        0        0     7661 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/multidispatcher.py
+-rw-r--r--   0        0        0        0 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/py.typed
+-rw-r--r--   0        0        0      651 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/server_defs.py
+-rwxr-xr-x   0        0        0    11622 2023-08-03 23:12:16.452261 flardl-0.0.7/src/flardl/stream_stats.py
+-rw-r--r--   0        0        0     9910 1970-01-01 00:00:00.000000 flardl-0.0.7/PKG-INFO
```

### Comparing `flardl-0.0.6/LICENSE` & `flardl-0.0.7/LICENSE`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/README.md` & `flardl-0.0.7/README.md`

 * *Files 13% similar despite different names*

```diff
@@ -25,88 +25,113 @@
 
 [![logo](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/flardl_bear.png)][logo license]
 
 [logo license]: https://raw.githubusercontent.com/hydrationdynamics/flardl/main/LICENSE.logo.txt
 
 ## Features
 
-_Flardl_ uses asynchronous I/O to adaptively download a list of files from one
-or more web servers. Speed advantages over synchronous downloads depends on
-network bandwidth, list length, and HTTP protocol, but for a gigabit
-connection using a list of 100 files, the download rate is
-**more than 300X higher** than for single-file downloads. Even a single
-server can saturate a gigabit connection, regardless of geographic
-location, but the number of files to get to saturation varies with
-latency. Use of multiple servers achieves saturated download rates on
-shorter lists of files, while providing better reliability in the face
-of varying upstream loads and service policies.
-
-## Theory
-
-Lengths of files generated by natural or human activity such as writing,
-coding, protein structures, or genomes tend to more-or-less follow
-**power-law distributions**, with a long tail to higher file sizes up to some maximum size
-in the collection. Operations on power-law distributions are more frequently
-covered in chemical rate theory than in computer-science approaches such as
-the leaky-bucket algorithm of queuing theory. A full model of the downloading
-process requires knowledge in advance which one doesn't generally have, such
-as exact file sizes, server policies, and network loads. I aim for a model
-that is both robust enough to work reliable in real situations and simple
-enough to be easily coded and understood.
-
-The nature of power-law processes is such that **mean values are nearly
-worthless**, because--unlike on normal distributions--means of runs drawn
-from a power-law distribution grow larger with number of samples. A few
-minutes spent with a mock downloading program (such as is included in
-_flardl_) will convince you that the total download time and therefore
-the mean downloading rate depends strongly on how many large-size
-outliers (let's call them
-"[Berthas](<https://en.wikipedia.org/wiki/Big_Bertha_(howitzer)>)")
-are included in your sample. Timings of algorithms that do
-near-simultaneous, asynchronous downloads will also depend very much on
-whether the Berthas are found at the beginning or the end of the
-stream and whether or not they happen to be doled out to the same server.
-A theories and algorithms based on overall times or mean rates won't
-work very well on the real-world power-law collections that are most
-important.
+_Flardl_ downloads lists of files from one or more servers
+using a novel adaptive asynchronous approach. Download rates
+are **typically more than 300X higherr** than synchronous
+utilities such as*curl*, while use of multiple servers
+provides better robustness in the face of varying network
+and server loads. Download rates depend on network bandwidth,
+latencies, list length, file sizes, and HTTP protocol used,
+but even a single server on another continent can usually
+saturate a gigabit connection after about 50 files using
+_flardl_.
+
+## Fishing Theory
+
+Collections of files generated by natural or human activity such
+as natural-language writing, protein structure determination,
+or genome sequencing tend to have **size distributions with
+long tails**. For collections with long-tail distributions, one
+finds many more examples of big files than of small files at
+a given additive distance above or below the peak (model) value.
+Examples of analytical forms of long-tail distributions include
+Zipf, power-law, and log-norm distributions. A real-world example
+of a long-tail distribution is shown in the figure below, which
+plots the file-size histogram for 1000 randomly-sampled examples
+CIF structure files from the [Protein Data Bank](https://rcsb.org) along with
+a kernel-density estimate and fits to log-normal and normal
+distributions.
+
+![sizedist](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/file_size_distribution.png)
+
+The effects of
+the big files in the long tail are frequently ignored in queuing
+algorithms.
+
+The nature of long-tail distributions is such that **mean values are nearly
+worthless** because--unlike on normal distributions--means of runs drawn
+from them grow larger with the size of the run. Because of the appreciable
+likelihood of drawing a really large file to be downloaded from a long-tail
+distribution, he total download time and therefore the mean downloading rate
+depends strongly on how many large-size outliers are included in your sample. Timings of algorithms that do
+If you are downloading multiple files simultaneously, the overall download
+time may also depend strongly on whether a large file happens to occur at
+the end of the list, causing an "overhang" of wwaiting for a single file.
+Theories and algorithms based on overall times or mean rates won't
+work very well on the long-tail distributions that often characterize
+real collections. T
 
 **Modal values are a good statistic for power-law distributions**, unlike
 means. To put that another way, the average download time $\overline{t_{dl}}$
 varies a lot
 between runs, but the _most-common_ download time
 $\tilde{t}_{dl}$ can be pretty
 consistent. The mode of file lengths and the mode of download bit rate
 are both quantities that are easy to estimate for a
 collection and a collection and rarely change. If one happens to select
 the biggest files for downloading, or if one happens to try downloading
 a long collection at the same time that someone is watching a high-bit-rate
 video on the same shared connection, then it's easy to adjust a bit
 for just that time.
 
+Here I propose a heuristic called **adaptive-depth queuing**
+that gives robust performance in real situations while being simple
+enough to be easily understood and coded.
+
 Even more than maximizing download rates, the highest priority must
 be to **avoid black-listing by a server**. Most public-facing servers
 have policies to recognize and defend against Denial-Of-Service (DOS)
 attacks. The response to a DOS event, at the very least, causes the server to
 dump your latest request, which is usually a minor nuisance
 as it can be retried later. Far worse is
 if the server responds by severely throttling further requests from your
-IP address, for a period of time, generally hours or sometime days.
+IP address for hours or sometime days.
 Worst of all, your IP address can get the "death penalty" and be put
 on a permanent blacklist that may require manual intervention for
-removal. You generally don't know the trigger levels for these policies.
+removal. You generally don't know thThe simplest
+possibility of le trigger levels for these policies.
 Worse still, it might not even be you. I have seen a practical class
-of 20 bioinformatics students brought to a complete halt
-by a 24-hour black-listing of the institution's IP address from which
-all traffic appeared to emanate by firewall policy. So, the simplest
-possibility of launching a large number of requests and letting the
-server sort it out is a poor strategy because it maximizes the chance
-of black-listing. Given that a single server can saturate a gigabit
-connection, given enough simultaneous downloads, it seems the best
-strategy is to keep the request-queue depth as low as possible to
-achieve that saturation. For those who are lucky enough to be on
+of 20 students brought to a complete halt
+by a server's 24-hour black-listing of the institution's IP address.
+
+Simply launching a large number of requests and letting the
+servers sort it out is a strategy that maximizes the chance
+of black-listing for two reasons. First, this strategy results in
+equal division of transfers without regard to varying transfer sizes or
+server latencies. Second,
+
+Given that a single server can saturate a gigabit
+connection, given enough simultaneous downloads, a better
+strategy is to **keep the total request-queue depth just high enough to
+achieve saturation**. This goal can be achieved by launching a large
+number of requests, up to some maximum permissible queue depth
+$Q_{\rm max}$ (either by guess or by previous knowledge of individual
+servers), during the server latency period when no transfers have been
+completed. As transfers are completed, one can then calculate the
+saturation bandwidth $B$ and
+the total-over-all-servers depth at which saturation was achieved,
+$Q_{\rm sat}$
+
+running the
+request For those who are lucky enough to be on
 a multi-gigabit connection, it's a good idea to limit the bandwidth
 to something you know the set of servers you are using won't complain
 about. It would be nice if one could query a server for an acceptable
 request queue depth which would guarantee no DOS response or other
 server throttling, but I have not seen such a mechanism implemented.
 
 ## Requirements
```

### Comparing `flardl-0.0.6/pyproject.toml` & `flardl-0.0.7/pyproject.toml`

 * *Files 18% similar despite different names*

```diff
@@ -1,41 +1,44 @@
 [tool.poetry]
 name = "flardl"
-version = "0.0.6"
+version = "0.0.7"
 description = "Flardl"
 authors = ["Joel Berendzen <joel@generisbio.com>"]
 license = "BSD-3-Clause"
 readme = "README.md"
 homepage = "https://github.com/hydrationdynamics/flardl"
 repository = "https://github.com/hydrationdynamics/flardl"
 documentation = "https://flardl.readthedocs.io"
 classifiers = [
-    "Development Status :: 2 - Pre-Alpha",
+    "Development Status :: 3 - Alpha",
     "Environment :: Console",
     "Intended Audience :: Developers",
     "License :: OSI Approved :: BSD License",
     "Natural Language :: English",
     "Operating System :: OS Independent",
     "Programming Language :: Python :: 3.9",
     "Programming Language :: Python :: 3.10",
     "Programming Language :: Python :: 3.11",
-    "Programming Language :: Python :: 3.12",
     "Topic :: Internet :: WWW/HTTP",
 ]
 keywords = [
     "downloads",
+    "asynchronous",
+    "high-performance",
+    "multi-dispatching",
+    "queueing",
     "adaptive",
     "federated"
 ]
 
 [tool.poetry.urls]
 Changelog = "https://github.com/hydrationdynamics/flardl/releases"
 
 [tool.poetry.dependencies]
-python = ">=3.9,<4"
+python = ">=3.9,<3.13"
 anyio = ">=3.7.0"
 attrs = "^22.2.0"
 httpx = {extras = ["http2"], version = ">=0.23.3"}
 loguru = ">=0.6.0"
 tqdm = ">=4.64.1"
 uvloop = { version = ">=0.17.0", markers = "sys_platform != 'win32'"}
 trio = "^0.22.0"
@@ -63,23 +66,28 @@
 safety = ">=1.10.3"
 sphinx = ">=4.3.2"
 sphinx-autobuild = ">=2021.3.14"
 typeguard = ">=2.13.3"
 xdoctest = {extras = ["colors"], version = ">=0.15.10"}
 myst-parser = {version = ">=0.16.1"}
 # test deps for this package
-loguru-mypy = ">=0.0.4"
 pandas = ">=1.4.0"
 pandas-stubs = ">=1.5.2.221124"
 pytest-datadir-mgr = ">1.3.1"
 pytest-trio = ">=0.8.0"
 numpy = ">=1.25.1"
 
+
+[tool.poetry.group.analysis.dependencies]
+seaborn = ">=0.12.2"
+scipy = ">=1.11.1"
+typer = ">=0.9.0"
+
 [tool.bandit]
-exclude_dirs = ["tests/", "noxfile.py"]
+exclude_dirs = ["tests/", "noxfile.py", "analysis/"]
 skips = ["B101", "B311"]
 
 [tool.coverage.paths]
 source = ["src", "*/site-packages"]
 tests = ["tests", "*/tests"]
 
 [tool.coverage.run]
@@ -88,15 +96,15 @@
 sigterm = true
 parallel = true
 
 [tool.coverage.report]
 show_missing = true
 
 [tool.cruft]
-skip = ["src/*", "tests/*", "poetry.lock"]
+skip = ["src/*", "tests/*", "analysis/", "poetry.lock"]
 
 [tool.isort]
 profile = "black"
 force_single_line = true
 lines_after_imports = 2
 
 [tool.mypy]
```

### Comparing `flardl-0.0.6/src/flardl/__init__.py` & `flardl-0.0.7/src/flardl/__init__.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/src/flardl/common.py` & `flardl-0.0.7/src/flardl/common.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/src/flardl/dict_to_indexed_list.py` & `flardl-0.0.7/src/flardl/dict_to_indexed_list.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/src/flardl/downloader.py` & `flardl-0.0.7/src/flardl/downloader.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Downloads as a MultiDispatcher worker class."""
-from __future__ import annotations
 
 import pathlib
 import sys
 from typing import Any
+from typing import Optional
 
 # third-party imports
 import anyio
 import httpx
 import loguru
 
 # module imports
@@ -21,16 +21,16 @@
 
 class StreamWorker:
     """Basic worker functions."""
 
     def __init__(
         self,
         worker_no: int,
-        logger: loguru.Logger,
-        output_dir: str | None,
+        logger: "loguru.Logger",
+        output_dir: Optional[str],
         quiet: bool,
         /,
         name: str,
         bw_limit_mbps: float = 0.0,
         queue_depth: int = 0,
         timeout_factor: float = 0.0,
         **kwargs,
```

### Comparing `flardl-0.0.6/src/flardl/instrumented_streams.py` & `flardl-0.0.7/src/flardl/instrumented_streams.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/src/flardl/multidispatcher.py` & `flardl-0.0.7/src/flardl/multidispatcher.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Dispatch work to multiple workers and collect results via AnyIO streams."""
-from __future__ import annotations
 
 import sys
 from typing import Any
+from typing import Optional
 
 # third-party imports
 import anyio
 import httpx
 import loguru
 from loguru import logger as mylogger
 
@@ -30,15 +30,15 @@
 
     def __init__(  # noqa: C901
         self,
         all_worker_defs: list[ServerDef],
         /,
         worker_list: list[str] | None = None,
         max_retries: int = DEFAULT_MAX_RETRIES,
-        logger: loguru.Logger | None = None,
+        logger: Optional["loguru.Logger"] = None,
         quiet: bool = False,
         history_len: int = 0,
         output_dir: str | None = None,
         mock: bool = False,
         runner: str = "production",
     ) -> None:
         """Save list of dispatchers."""
```

### Comparing `flardl-0.0.6/src/flardl/server_defs.py` & `flardl-0.0.7/src/flardl/server_defs.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/src/flardl/stream_stats.py` & `flardl-0.0.7/src/flardl/stream_stats.py`

 * *Files identical despite different names*

### Comparing `flardl-0.0.6/PKG-INFO` & `flardl-0.0.7/PKG-INFO`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 Metadata-Version: 2.1
 Name: flardl
-Version: 0.0.6
+Version: 0.0.7
 Summary: Flardl
 Home-page: https://github.com/hydrationdynamics/flardl
 License: BSD-3-Clause
-Keywords: downloads,adaptive,federated
+Keywords: downloads,asynchronous,high-performance,multi-dispatching,queueing,adaptive,federated
 Author: Joel Berendzen
 Author-email: joel@generisbio.com
-Requires-Python: >=3.9,<4
-Classifier: Development Status :: 2 - Pre-Alpha
+Requires-Python: >=3.9,<3.13
+Classifier: Development Status :: 3 - Alpha
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
-Classifier: Programming Language :: Python :: 3.12
 Classifier: Topic :: Internet :: WWW/HTTP
 Requires-Dist: anyio (>=3.7.0)
 Requires-Dist: attrs (>=22.2.0,<23.0.0)
 Requires-Dist: httpx[http2] (>=0.23.3)
 Requires-Dist: loguru (>=0.6.0)
 Requires-Dist: tqdm (>=4.64.1)
 Requires-Dist: trio (>=0.22.0,<0.23.0)
@@ -59,88 +58,113 @@
 
 [![logo](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/flardl_bear.png)][logo license]
 
 [logo license]: https://raw.githubusercontent.com/hydrationdynamics/flardl/main/LICENSE.logo.txt
 
 ## Features
 
-_Flardl_ uses asynchronous I/O to adaptively download a list of files from one
-or more web servers. Speed advantages over synchronous downloads depends on
-network bandwidth, list length, and HTTP protocol, but for a gigabit
-connection using a list of 100 files, the download rate is
-**more than 300X higher** than for single-file downloads. Even a single
-server can saturate a gigabit connection, regardless of geographic
-location, but the number of files to get to saturation varies with
-latency. Use of multiple servers achieves saturated download rates on
-shorter lists of files, while providing better reliability in the face
-of varying upstream loads and service policies.
-
-## Theory
-
-Lengths of files generated by natural or human activity such as writing,
-coding, protein structures, or genomes tend to more-or-less follow
-**power-law distributions**, with a long tail to higher file sizes up to some maximum size
-in the collection. Operations on power-law distributions are more frequently
-covered in chemical rate theory than in computer-science approaches such as
-the leaky-bucket algorithm of queuing theory. A full model of the downloading
-process requires knowledge in advance which one doesn't generally have, such
-as exact file sizes, server policies, and network loads. I aim for a model
-that is both robust enough to work reliable in real situations and simple
-enough to be easily coded and understood.
-
-The nature of power-law processes is such that **mean values are nearly
-worthless**, because--unlike on normal distributions--means of runs drawn
-from a power-law distribution grow larger with number of samples. A few
-minutes spent with a mock downloading program (such as is included in
-_flardl_) will convince you that the total download time and therefore
-the mean downloading rate depends strongly on how many large-size
-outliers (let's call them
-"[Berthas](<https://en.wikipedia.org/wiki/Big_Bertha_(howitzer)>)")
-are included in your sample. Timings of algorithms that do
-near-simultaneous, asynchronous downloads will also depend very much on
-whether the Berthas are found at the beginning or the end of the
-stream and whether or not they happen to be doled out to the same server.
-A theories and algorithms based on overall times or mean rates won't
-work very well on the real-world power-law collections that are most
-important.
+_Flardl_ downloads lists of files from one or more servers
+using a novel adaptive asynchronous approach. Download rates
+are **typically more than 300X higherr** than synchronous
+utilities such as*curl*, while use of multiple servers
+provides better robustness in the face of varying network
+and server loads. Download rates depend on network bandwidth,
+latencies, list length, file sizes, and HTTP protocol used,
+but even a single server on another continent can usually
+saturate a gigabit connection after about 50 files using
+_flardl_.
+
+## Fishing Theory
+
+Collections of files generated by natural or human activity such
+as natural-language writing, protein structure determination,
+or genome sequencing tend to have **size distributions with
+long tails**. For collections with long-tail distributions, one
+finds many more examples of big files than of small files at
+a given additive distance above or below the peak (model) value.
+Examples of analytical forms of long-tail distributions include
+Zipf, power-law, and log-norm distributions. A real-world example
+of a long-tail distribution is shown in the figure below, which
+plots the file-size histogram for 1000 randomly-sampled examples
+CIF structure files from the [Protein Data Bank](https://rcsb.org) along with
+a kernel-density estimate and fits to log-normal and normal
+distributions.
+
+![sizedist](https://raw.githubusercontent.com/hydrationdynamics/flardl/main/docs/_static/file_size_distribution.png)
+
+The effects of
+the big files in the long tail are frequently ignored in queuing
+algorithms.
+
+The nature of long-tail distributions is such that **mean values are nearly
+worthless** because--unlike on normal distributions--means of runs drawn
+from them grow larger with the size of the run. Because of the appreciable
+likelihood of drawing a really large file to be downloaded from a long-tail
+distribution, he total download time and therefore the mean downloading rate
+depends strongly on how many large-size outliers are included in your sample. Timings of algorithms that do
+If you are downloading multiple files simultaneously, the overall download
+time may also depend strongly on whether a large file happens to occur at
+the end of the list, causing an "overhang" of wwaiting for a single file.
+Theories and algorithms based on overall times or mean rates won't
+work very well on the long-tail distributions that often characterize
+real collections. T
 
 **Modal values are a good statistic for power-law distributions**, unlike
 means. To put that another way, the average download time $\overline{t_{dl}}$
 varies a lot
 between runs, but the _most-common_ download time
 $\tilde{t}_{dl}$ can be pretty
 consistent. The mode of file lengths and the mode of download bit rate
 are both quantities that are easy to estimate for a
 collection and a collection and rarely change. If one happens to select
 the biggest files for downloading, or if one happens to try downloading
 a long collection at the same time that someone is watching a high-bit-rate
 video on the same shared connection, then it's easy to adjust a bit
 for just that time.
 
+Here I propose a heuristic called **adaptive-depth queuing**
+that gives robust performance in real situations while being simple
+enough to be easily understood and coded.
+
 Even more than maximizing download rates, the highest priority must
 be to **avoid black-listing by a server**. Most public-facing servers
 have policies to recognize and defend against Denial-Of-Service (DOS)
 attacks. The response to a DOS event, at the very least, causes the server to
 dump your latest request, which is usually a minor nuisance
 as it can be retried later. Far worse is
 if the server responds by severely throttling further requests from your
-IP address, for a period of time, generally hours or sometime days.
+IP address for hours or sometime days.
 Worst of all, your IP address can get the "death penalty" and be put
 on a permanent blacklist that may require manual intervention for
-removal. You generally don't know the trigger levels for these policies.
+removal. You generally don't know thThe simplest
+possibility of le trigger levels for these policies.
 Worse still, it might not even be you. I have seen a practical class
-of 20 bioinformatics students brought to a complete halt
-by a 24-hour black-listing of the institution's IP address from which
-all traffic appeared to emanate by firewall policy. So, the simplest
-possibility of launching a large number of requests and letting the
-server sort it out is a poor strategy because it maximizes the chance
-of black-listing. Given that a single server can saturate a gigabit
-connection, given enough simultaneous downloads, it seems the best
-strategy is to keep the request-queue depth as low as possible to
-achieve that saturation. For those who are lucky enough to be on
+of 20 students brought to a complete halt
+by a server's 24-hour black-listing of the institution's IP address.
+
+Simply launching a large number of requests and letting the
+servers sort it out is a strategy that maximizes the chance
+of black-listing for two reasons. First, this strategy results in
+equal division of transfers without regard to varying transfer sizes or
+server latencies. Second,
+
+Given that a single server can saturate a gigabit
+connection, given enough simultaneous downloads, a better
+strategy is to **keep the total request-queue depth just high enough to
+achieve saturation**. This goal can be achieved by launching a large
+number of requests, up to some maximum permissible queue depth
+$Q_{\rm max}$ (either by guess or by previous knowledge of individual
+servers), during the server latency period when no transfers have been
+completed. As transfers are completed, one can then calculate the
+saturation bandwidth $B$ and
+the total-over-all-servers depth at which saturation was achieved,
+$Q_{\rm sat}$
+
+running the
+request For those who are lucky enough to be on
 a multi-gigabit connection, it's a good idea to limit the bandwidth
 to something you know the set of servers you are using won't complain
 about. It would be nice if one could query a server for an acceptable
 request queue depth which would guarantee no DOS response or other
 server throttling, but I have not seen such a mechanism implemented.
 
 ## Requirements
```

