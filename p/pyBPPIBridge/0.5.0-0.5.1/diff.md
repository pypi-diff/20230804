# Comparing `tmp/pyBPPIBridge-0.5.0-py3-none-any.whl.zip` & `tmp/pyBPPIBridge-0.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,51 +1,52 @@
-Zip file size: 44306 bytes, number of entries: 49
+Zip file size: 46964 bytes, number of entries: 50
 -rw-rw-rw-  2.0 fat      154 b- defN 23-Jul-25 17:11 bppibridge.py
 -rw-rw-rw-  2.0 fat      413 b- defN 23-Jul-25 17:11 bppibridgesq.py
 -rw-rw-rw-  2.0 fat      569 b- defN 23-Aug-01 14:12 bppibridge/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 config/__init__.py
 -rw-rw-rw-  2.0 fat     4112 b- defN 23-Jul-25 17:11 config/appConfig.py
 -rw-rw-rw-  2.0 fat     3059 b- defN 23-Aug-02 07:20 config/cmdLineConfig.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/__init__.py
 -rw-rw-rw-  2.0 fat     3437 b- defN 23-Aug-01 12:53 pipelines/pipeline.py
--rw-rw-rw-  2.0 fat     4644 b- defN 23-Aug-01 14:13 pipelines/pipelineFactory.py
+-rw-rw-rw-  2.0 fat     4992 b- defN 23-Aug-04 15:03 pipelines/pipelineFactory.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/__init__.py
 -rw-rw-rw-  2.0 fat     5058 b- defN 23-Aug-01 08:49 pipelines/bppi/bppiPipeline.py
 -rw-rw-rw-  2.0 fat     1384 b- defN 23-Aug-01 08:08 pipelines/bppi/uploadConfig.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/project/__init__.py
 -rw-rw-rw-  2.0 fat      969 b- defN 23-Aug-01 08:08 pipelines/bppi/project/bppiApiProjectWrapper.py
 -rw-rw-rw-  2.0 fat      523 b- defN 23-Aug-01 08:08 pipelines/bppi/project/bppiProject.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/__init__.py
 -rw-rw-rw-  2.0 fat     8963 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/bppiApiRepositoryWrapper.py
 -rw-rw-rw-  2.0 fat     9166 b- defN 23-Aug-01 12:53 pipelines/bppi/repository/bppiRepository.py
 -rw-rw-rw-  2.0 fat     2195 b- defN 23-Aug-01 08:08 pipelines/bppi/repository/repConfig.py
+-rw-rw-rw-  2.0 fat    12364 b- defN 23-Aug-04 15:11 pipelines/buslogs/blueprismLogs.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/project/__init__.py
 -rw-rw-rw-  2.0 fat      679 b- defN 23-Aug-01 08:08 pipelines/readers/Reader.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-25 17:11 pipelines/readers/__init__.py
--rw-rw-rw-  2.0 fat     9613 b- defN 23-Aug-02 08:30 pipelines/readers/bpAPIReader.py
+-rw-rw-rw-  2.0 fat     9665 b- defN 23-Aug-04 08:29 pipelines/readers/bpAPIReader.py
 -rw-rw-rw-  2.0 fat     1052 b- defN 23-Aug-01 08:08 pipelines/readers/csvFileReader.py
 -rw-rw-rw-  2.0 fat     1105 b- defN 23-Aug-01 08:08 pipelines/readers/excelFileReader.py
 -rw-rw-rw-  2.0 fat     1422 b- defN 23-Aug-02 12:19 pipelines/readers/odbcReader.py
 -rw-rw-rw-  2.0 fat     3913 b- defN 23-Aug-01 08:08 pipelines/readers/sapRFCTableReader.py
 -rw-rw-rw-  2.0 fat     3989 b- defN 23-Aug-01 08:08 pipelines/readers/xesFileReader.py
 -rw-rw-rw-  2.0 fat     2209 b- defN 23-Aug-02 11:46 pipelines/readers/builders/SQLBuilder.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-01 08:08 pipelines/readers/builders/__init__.py
--rw-rw-rw-  2.0 fat     3676 b- defN 23-Aug-02 12:06 pipelines/readers/builders/blueprismSQLBuilder.py
+-rw-rw-rw-  2.0 fat     3648 b- defN 23-Aug-04 14:47 pipelines/readers/builders/blueprismSQLBuilder.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-12 06:39 pipelines/repository/__init__.py
--rw-rw-rw-  2.0 fat     2164 b- defN 23-Aug-02 08:23 pipelines/repository/bppiPLRBluePrismApi.py
--rw-rw-rw-  2.0 fat    12377 b- defN 23-Aug-02 12:36 pipelines/repository/bppiPLRBluePrismRepo.py
+-rw-rw-rw-  2.0 fat     3579 b- defN 23-Aug-04 15:12 pipelines/repository/bppiPLRBluePrismApi.py
+-rw-rw-rw-  2.0 fat     6970 b- defN 23-Aug-04 14:51 pipelines/repository/bppiPLRBluePrismRepo.py
 -rw-rw-rw-  2.0 fat     1338 b- defN 23-Aug-01 12:54 pipelines/repository/bppiPLRCSVFile.py
 -rw-rw-rw-  2.0 fat      788 b- defN 23-Aug-01 12:57 pipelines/repository/bppiPLRChorusExtract.py
 -rw-rw-rw-  2.0 fat     1467 b- defN 23-Aug-01 12:57 pipelines/repository/bppiPLRExcelFile.py
 -rw-rw-rw-  2.0 fat     1888 b- defN 23-Aug-02 11:48 pipelines/repository/bppiPLRODBC.py
 -rw-rw-rw-  2.0 fat     2590 b- defN 23-Aug-01 13:43 pipelines/repository/bppiPLRSAPRfcTable.py
 -rw-rw-rw-  2.0 fat     1303 b- defN 23-Aug-01 12:58 pipelines/repository/bppiPLRXESFile.py
 -rw-rw-rw-  2.0 fat      555 b- defN 23-Jul-25 17:11 utils/__init__.py
--rw-rw-rw-  2.0 fat     8001 b- defN 23-Aug-02 07:13 utils/constants.py
+-rw-rw-rw-  2.0 fat     8529 b- defN 23-Aug-04 14:46 utils/constants.py
 -rw-rw-rw-  2.0 fat     2071 b- defN 23-Jul-25 17:11 utils/log.py
--rw-rw-rw-  2.0 fat     1091 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     3868 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     4303 b- defN 23-Aug-02 13:23 pyBPPIBridge-0.5.0.dist-info/RECORD
-49 files, 116294 bytes uncompressed, 37352 bytes compressed:  67.9%
+-rw-rw-rw-  2.0 fat     1091 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     3868 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4394 b- defN 23-Aug-04 16:46 pyBPPIBridge-0.5.1.dist-info/RECORD
+50 files, 125657 bytes uncompressed, 39866 bytes compressed:  68.3%
```

## zipnote {}

```diff
@@ -51,14 +51,17 @@
 
 Filename: pipelines/bppi/repository/bppiRepository.py
 Comment: 
 
 Filename: pipelines/bppi/repository/repConfig.py
 Comment: 
 
+Filename: pipelines/buslogs/blueprismLogs.py
+Comment: 
+
 Filename: pipelines/project/__init__.py
 Comment: 
 
 Filename: pipelines/readers/Reader.py
 Comment: 
 
 Filename: pipelines/readers/__init__.py
@@ -123,26 +126,26 @@
 
 Filename: utils/constants.py
 Comment: 
 
 Filename: utils/log.py
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/LICENSE
+Filename: pyBPPIBridge-0.5.1.dist-info/LICENSE
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/METADATA
+Filename: pyBPPIBridge-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/WHEEL
+Filename: pyBPPIBridge-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/entry_points.txt
+Filename: pyBPPIBridge-0.5.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/top_level.txt
+Filename: pyBPPIBridge-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: pyBPPIBridge-0.5.0.dist-info/RECORD
+Filename: pyBPPIBridge-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pipelines/pipelineFactory.py

```diff
@@ -39,18 +39,19 @@
 			int: Number of rows loaded
 		"""
 		try:
 			# INSTANCIATE ONLY THE NEEDED CLASS / DATA SOURCE TYPE
 			self.log.info("BPPI Bridge initialisation ...")
 			pipeline = self.create()
 			if (pipeline == None):
-				raise Exception ("The Data pipeline cannot be created")
+				raise Exception ("The Data pipeline has not been created successfully")
 		except Exception as e:
-			self.log.error("Error> pipelineFactory.process(): The bridge cannot be initialized: {}".format(str(e)))
-	
+			self.log.error("pipelineFactory.process(): The BPPI bridge cannot be initialized: {}".format(str(e)))
+			return
+		
 		return self.execute(pipeline=pipeline)
 	
 	def execute(self, pipeline):
 		""" Execute the pipeline
 		Returns:
 			int: Number of rows read
 			int: Number of rows transformed
@@ -86,15 +87,15 @@
 			else:
 				self.log.error("pipelineFactory.createAndExecute(): The Data pipeline has not been initialized properly")
 			
 			pipeline.terminate()
 			return E_counts, T_counts, L_counts
 		
 		except Exception as e:
-			pipeline.log.error("pipelineFactory.createAndExecute(): Error when processing the data: {}".format(str(e)))
+			self.log.error("pipelineFactory.createAndExecute(): Error when processing the data: {}".format(str(e)))
 			return E_counts, T_counts, L_counts
 
 	def create(self) -> pipeline:
 		""" This function dynamically instanciate the right data pipeline (manages ETL) class to create a pipeline object. 
 			This to avoid in loading all the connectors (if any of them failed for example) when making a global import, 
 			by this way only the needed import is done on the fly
 			Args:
@@ -103,16 +104,21 @@
 			Returns:
 				Object: Data Source Object
 		"""
 		try:
 			# Get the pipeline class to instantiate from the config
 			pipelinePath = self.config.getParameter(C.PARAM_PIPELINE_PATH, C.PIPELINE_FOLDER)
 			pipelineClass = self.config.getParameter(C.PARAM_PIPELINE_CLASSNAME, C.PIPELINE_FOLDER)
-
+			fullClassPath = pipelinePath + "." + pipelineClass
+			
 			# Instantiate the pipeline object
-			datasourceObject = importlib.import_module(pipelinePath + "." + pipelineClass)
+			self.log.debug("pipelineFactory.create(): Import module -> {}".format(fullClassPath))
+			datasourceObject = importlib.import_module(fullClassPath)
+			self.log.debug("pipelineFactory.create(): Module {} imported, instantiate the class".format(fullClassPath))
 			pipelineClass = getattr(datasourceObject, pipelineClass)
-			return pipelineClass(self.config, self.log)
+			pipelineObject = pipelineClass(self.config, self.log)
+			self.log.info("Pipeline created successfully")
+			return pipelineObject
 		
 		except Exception as e:
 			self.log.error("pipelineFactory.create(): Error when loading the Data Source Factory: {}".format(str(e)))
 			return None
```

## pipelines/readers/bpAPIReader.py

```diff
@@ -3,16 +3,18 @@
 __license__ = "MIT"
 
 import utils.constants as C
 import pandas as pd
 from .Reader import Reader 
 import requests 
 import urllib.parse
+import warnings
 
 AUTH_TOKEN_SUFFIX_URL = "/connect/token"
+warnings.filterwarnings('ignore')
 
 class bpAPIReader(Reader):
 
     def setConnectionParams(self, urlAuth, urlApi, sslCheck, pageSize, clientID, secret, bpProcessName):
         self.__urlAuth = urlAuth
         self.__sslCheck = sslCheck
         self.__pageSize = pageSize
```

## pipelines/readers/builders/blueprismSQLBuilder.py

```diff
@@ -37,24 +37,24 @@
             if (self.__includeVBO != C.YES):
                 novbo = C.BPLOG_PROCESSNAME_COL + " IS NULL"
 
             # Date Filtering and/or DELTA vs FULL
             if (self.__deltaDate != C.EMPTY):
                 self.log.info("DELTA Load requested - from <" + str(self.__deltaDate) + ">")
                 # DELTA LOAD (get date from file first)
-                deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__deltaDate + "'"
+                deltasql = " FORMAT(" + C.BPLOG_FILTERDATE_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__deltaDate + "'"
             else:
                 self.log.info("FULL Load requested")
                 # FULL LOAD / Add the delta extraction filters if required (-fromdate and/or -todate filled)
                 if ((self.__fromDate != C.EMPTY) and (self.__toDate != C.EMPTY)):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') BETWEEN '" + self.__fromDate + "' AND '" + self.__toDate + "'"
+                    deltasql = " FORMAT(" + C.BPLOG_FILTERDATE_COL + ",'yyyy-MM-dd HH:mm:ss') BETWEEN '" + self.__fromDate + "' AND '" + self.__toDate + "'"
                 elif (self.__fromDate != C.EMPTY):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__fromDate + "'"
+                    deltasql = " FORMAT(" + C.BPLOG_FILTERDATE_COL + ",'yyyy-MM-dd HH:mm:ss') >= '" + self.__fromDate + "'"
                 elif (self.__toDate != C.EMPTY):
-                    deltasql = " FORMAT(LOG." + C.BPLOG_STARTDATETIME_COL + ",'yyyy-MM-dd HH:mm:ss') <= '" + self.__toDate + "'"
+                    deltasql = " FORMAT(" + C.BPLOG_FILTERDATE_COL + ",'yyyy-MM-dd HH:mm:ss') <= '" + self.__toDate + "'"
 
             # BP Logs in unicode ? (default no)
             if (self.__unicode == C.YES):
                 tablelog = C.BPLOG_LOG_UNICODE
             else:
                 tablelog = C.BPLOG_LOG_NONUNICODE
```

## pipelines/repository/bppiPLRBluePrismApi.py

```diff
@@ -2,27 +2,38 @@
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import utils.constants as C
 from pipelines.bppi.repository.bppiRepository import bppiRepository
 import pandas as pd
 from pipelines.readers.bpAPIReader import bpAPIReader
+from pipelines.buslogs.blueprismLogs import blueprismLogs
 
 BP_MANDATORY_PARAM_LIST = [C.PARAM_BPPITOKEN, 
                            C.PARAM_BPPIURL, 
                            C.PARAM_BPPROCESSNAME,
                            C.PARAM_BPAPI_CLIENT_ID,
                            C.PARAM_BPAPI_SECRET,
                            C.PARAM_BPAPI_AUTH_URL]
 
 """ Manages the Blue Prism API extraction interface
     Class hierarchy:
     - bppiapi.bppiPipeline
         - bppiapi.repository.bppiRepository
             - pipelines.repository.bppiPLRBluePrismApi
+    Columns read from the API:
+    (*) logId
+    (*) SessionID
+    (*) stageName
+    (*) stageType
+    (*) result
+    (*) resourceStartTime
+    (*) ResourceName
+    hasParameters
+    status
 """
 class bppiPLRBluePrismApi(bppiRepository):
     @property
     def mandatoryParameters(self) -> str:
         return BP_MANDATORY_PARAM_LIST
 
     def extract(self) -> pd.DataFrame: 
@@ -41,8 +52,32 @@
                                     urlAuth=self.config.getParameter(C.PARAM_BPAPI_AUTH_URL, C.EMPTY))
             if (not api.read()):
                 raise Exception("Error while accessing the Blue Prism API")
             return api.content
         
         except Exception as e:
             self.log.error("Extract() Error -> " + str(e))
-            return super().extract()
+            return super().extract()
+        
+    def transform(self, df) -> pd.DataFrame:
+        """Alter the collected data (from the BP Repository) by managing the attributes (stored in a XML format)
+        Args:
+            df (pd.DataFrame): Data source
+        Returns:
+            pd.DataFrame: Altered dataset with the selected parameters as new columns
+        """
+        try:
+            logs = blueprismLogs(dfLogs=df, log=self.log)
+
+            # Add the stage identifier / event mapping needs
+            logs.createStageID()
+
+            # Change the event to map by default if not filled out (surcharge the events.eventcolumn INI parameter)
+            if (self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.EMPTY) == C.EMPTY and logs.checkField(C.COL_STAGE_ID)):
+                self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.COL_STAGE_ID)
+
+            # Filter and/or update the event names if needed/configured
+            return super().transform(logs.content)
+        
+        except Exception as e:
+            self.log.error("bppiPLRBluePrismApi.transform() -> Unable to update the data " + str(e))
+            return super().transform(df)
```

## pipelines/repository/bppiPLRBluePrismRepo.py

```diff
@@ -1,35 +1,46 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import utils.constants as C
 from pipelines.repository.bppiPLRODBC import bppiPLRODBC
 import pandas as pd
-import xml.etree.ElementTree as ET
-import warnings
-import numpy as np
 from pipelines.readers.builders.blueprismSQLBuilder import blueprismSQLBuilder
+from pipelines.buslogs.blueprismLogs import blueprismLogs
 import datetime
 
-warnings.filterwarnings('ignore')
 CANCEL_SQL_FILTER = "1=1"
 BP_MANDATORY_PARAM_LIST = [C.PARAM_CONNECTIONSTRING, 
                            C.PARAM_BPPITOKEN, 
                            C.PARAM_BPPIURL, 
                            C.PARAM_BPPROCESSNAME]
 
 """ Manages the Blue Prism Repository extraction interface
     Class hierarchy:
     - bppiapi.bppiPipeline
         - bppiapi.repository.bppiRepository
             - pipelines.repository.bppiPLRCSVFile
                 - pipelines.repository.bppiPLRODBC
                     - pipelines.repository.bppiPLRBluePrismRepo
+    READ data from repository columns (*) means common cols with API:
+        (*) logId: log identifier -> C.BPLOG_FIELD_LOGID
+        (*) SessionID: session identifier -> C.BPLOG_FIELD_SESSIONID
+        (*) stageName: stage name -> C.BPLOG_STAGENAME_COL
+        (*) stagetype: stage type -> C.BPLOG_STAGETYPE_COL
+        (*) result: result of exec -> C.BPLOG_RESULT_COL
+        (*) resourceStartTime: start time -> C.BPLOG_STARTDATETIME_COL
+        (*) ResourceName: DW worker name -> C.BPLOG_RESOURCENAME_COL
+        actionname: vbo action name
+        pagename: page name (if process)
+        attributexml : Attributes (bp variables in a XML format)
+        OBJECT_TYPE: VBO or PROC
+        OBJECT_NAME: Process or VBO Name
 """
+
 class bppiPLRBluePrismRepo(bppiPLRODBC):
     @property
     def mandatoryParameters(self) -> str:
         return BP_MANDATORY_PARAM_LIST
 
     def __getDeltaTag(self):
         """ Get the last load date to use for the delta loading (when requested)
@@ -39,34 +50,34 @@
         if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
             filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
             try:
                 with open(filedelta, "r") as file:
                     fromdate = file.read()
                 return fromdate
             except:
-                self.log.error("__getDeltaLoadLastDate() -> Unable to read/get the tagged delta date")
+                self.log.error("bppiPLRBluePrismRepo.__getDeltaLoadLastDate() -> Unable to read/get the tagged delta date")
                 return C.EMPTY
         else:
             return C.EMPTY
 
     def __updDeltaTag(self):
         """ Update the date for the next delta load
         """
         if (self.config.getParameter(C.PARAM_BPDELTA, C.NO) == C.YES):
             try:
                 filedelta = self.config.getParameter(C.PARAM_BPDELTA_FILE, C.BP_DEFAULT_DELTAFILE)
                 with open(filedelta, "w") as file: # store in the delta file the latest delta load 
                     file.write(datetime.datetime.now().strftime(C.BP_DELTADATE_FMT))
             except:
-                self.log.error("__updDeltaLoadLastDate() -> Unable to write the tagged new delta date")
+                self.log.error("bppiPLRBluePrismRepo.__updDeltaLoadLastDate() -> Unable to write the tagged new delta date")
 
     @property
     def query(self) -> str:
         """Build the SQL Query to get the BP logs against the BP repository
-            The BP Logs SQL qeury is stored in the bp.config file and can be customized with several args:
+            The BP Logs SQL query is stored in the bp.config file and can be customized with several args:
                 * {attrxml}: Name of the INPUT/OUTPUT attributes columns (XML format)
                 * {processname}: Process Name in Blue Prism
                 * {stagetypefilter}: list of stage to filter out
                 * {delta}: Delta loading condition on datetime (Between or < >)
                 * {tablelog}: Name of the Log table (unicode or not unicode)
         Returns:
             str: built SQL Query
@@ -86,137 +97,43 @@
                                            toDate=self.config.getParameter(C.PARAM_TODATE),
                                            deltaDate=lastDeltaDate)
             sql = sqlBuilder.build()
             # Update the date for the next delta load
             self.__updDeltaTag()
             return sql
         except Exception as e:
-            self.log.error("__buildQuery() -> Unable to build the Blue Prism Query " + str(e))
+            self.log.error("bppiPLRBluePrismRepo.__buildQuery() -> Unable to build the Blue Prism Query " + str(e))
             return C.EMPTY
-        
-    def __parseAttrs(self, logid, attribute, dfattributes) -> pd.DataFrame:
-        """ Parse the attributexml field and extract (only) the text data (not the collection)
-        Args:
-            logid (str): ID of the log line (for later merge)
-            attribute (str): attributexml value (XML format)
-            dfattributes (DataFrame): Dataframe with tne incremental parameters added into
 
-        Returns:
-            pd.DataFrame: _description_
-        """
-        try:
-            #    Blue Prism Log Format expected:
-            #    <parameters>
-            #        <inputs>
-            #            <input name="Nom" type="text" value="Benoit Cayla" />
-            #            ...
-            #        </inputs>
-            #        <outputs>
-            #            <output name="Contact Form" type="flag" value="True" />
-            #            ...
-            #        </outputs>
-            #    </parameters>
-            root = ET.fromstring(attribute)
-            if (root.tag == "parameters"):
-                for input in root.findall("./inputs/input"):
-                    if (input.attrib["type"] == "text"):    # only get the text input parameters
-                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
-                                                                'Name' : input.attrib["name"], 
-                                                                'value' :input.attrib["value"], 
-                                                                'in_out' : 'I'}, index=[0])
-                        dfattributes = pd.concat([dfattributes, df_new_row])
-                for output in root.findall("./outputs/output"):
-                    if (output.attrib["type"] == "text"):    # only get the text output parameters
-                        df_new_row = pd.DataFrame.from_records({'logid': logid, 
-                                                                'Name' : output.attrib["name"], 
-                                                                'value' :output.attrib["value"], 
-                                                                'in_out' : 'O'}, index=[0])
-                        dfattributes = pd.concat([dfattributes, df_new_row]) 
-            return dfattributes
-        except Exception as e:
-            self.log.error("__parseAttrs() -> Unable to parse the BP Attribute " + str(e))
-            return dfattributes
-
-    def __getAttributesFromLogs(self, df) -> pd.DataFrame:
-        """Extract the logs (especially the parameters from the logs which are stored in XML format)
-            Note: if no parameters in the list, no import
-        Args:
-            df (Dataframe): Dataframe with the logs
-            config (bppiapi.appConfig): list of parameters from the INI file
-        Returns:
-            DataFrame: logs altered with parameters
-        """
-        try:
-            parameters = self.config.getParameter(C.PARAM_BPPARAMSATTR, C.EMPTY)
-            # Manage the IN/OUT parameters from the logs
-            if (len(parameters) > 0):
-                # Extract the input and output parameters
-                self.log.info("Extract the input and output parameters")
-                dfattributes = pd.DataFrame(columns= ["logid", "Name", "value", "in_out"])
-                for index, row in df.iterrows():
-                    if (row[C.BPLOG_ATTRIBUTE_COL] != None):
-                        dfattributes = self.__parseAttrs(row["logid"], row[C.BPLOG_ATTRIBUTE_COL], dfattributes)
-                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
-                # Only keep the desired parameters
-                self.log.debug("Filter out the desired parameters")
-                # Build the filter with the parameters list
-                params = [ "\"" + x + "\"" for x in parameters.split(",") ]
-                paramQuery = "Name in (" + ",".join(params) + ")"
-                dfattributes = dfattributes.query(paramQuery)
-                self.log.debug("Number of attributes found: {}".format(str(dfattributes.shape[0])))
-                # Pivot the parameter values to create one new column per parameter
-                self.log.info("Build the final dataset with the desired parameters")
-                # add the IN or OUT parameter (the commented line below creates 2 differents parameters if the same param for IN and OUT)
-                dfattributes['FullName'] = dfattributes['Name']
-                dfattributesInCols = pd.pivot_table(dfattributes, values='value', index=['logid'], columns=['FullName'], aggfunc=np.sum, fill_value="")
-                dfattributesInCols.reset_index()
-                # Merge the Dataframes
-                dffinal = df.merge(dfattributesInCols, on="logid", how='left')
-                dffinal = dffinal.drop(C.BPLOG_ATTRIBUTE_COL, axis=1)
-                return dffinal
-            else:
-                self.log.info("No parameters required in the configuration file")
-                return df
-            
-        except Exception as e:
-            self.log.error("__getAttributesFromLogs() -> Unable to get attributes from the Blue Prism logs " + str(e))
-            return df
-    
     def transform(self, df) -> pd.DataFrame:
         """Alter the collected data (from the BP Repository) by managing the attributes (stored in a XML format)
         Args:
             df (pd.DataFrame): Data source
         Returns:
             pd.DataFrame: Altered dataset with the selected parameters as new columns
         """
-
         try:
+            logs = blueprismLogs(dfLogs=df, log=self.log)
+
             # Filter out the df by selecting only the Start & End (main page / process) stages if requested
             if (self.config.getParameter(C.PARAM_BPFILTERSTEND) == C.YES):
                 mainpage = self.config.getParameter(C.PARAM_BPMAINPROCESSPAGE, C.BP_MAINPAGE_DEFAULT) 
-                # Remove the logs with stagename = "End" outside the "Main Page"
-                oldCount = df.shape[0]
-                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_END) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
-                self.log.warning("{} records have been removed (No <End> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
-                # Remove the logs with stagename = "Start" outside the "Main Page"
-                oldCount = df.shape[0] 
-                df = df[~((df[C.BPLOG_STAGENAME_COL] == C.BP_STAGE_START) & (df[C.BPLOG_PAGENAME_COL] != mainpage))]
-                self.log.warning("{} records have been removed (No <Start> stage outside the Main Process Page)".format(oldCount - df.shape[0]))
+                logs.removeStartEndStages(mainpage)
             
             # Get the attributes from the BP logs
-            df = self.__getAttributesFromLogs(df)
+            logs.addAttributes(self.config.getParameter(C.PARAM_BPPARAMSATTR))
+
+            # Add the stage identifier / event mapping needs
+            logs.createStageID()
 
-            # Create a new col OBJECT_TAB with the page name or the VBO action
-            df[C.COL_OBJECT_TAB] = df.apply(lambda row: row["pagename"] if row["pagename"] != None else row["actionname"], axis=1)
-            # Create the unique stage Identifier: STAGE_ID: STAGE_ID format: {VBO|PROC}/{Process or Object Name}/{Process Page or VBO Action}/{Stage name}
-            df[C.COL_STAGE_ID] = df[['OBJECT_TYPE', 'OBJECT_NAME', C.COL_OBJECT_TAB, 'stagename']].agg('/'.join, axis=1)
             # Change the event to map by default if not filled out (surcharge the events.eventcolumn INI parameter)
-            if (self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.EMPTY) == C.EMPTY):
+            if (self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.EMPTY) == C.EMPTY and logs.checkField(C.COL_STAGE_ID)):
                 self.config.setParameter(C.PARAM_EVENTMAPTABLE, C.COL_STAGE_ID)
 
+            logs.dropFields([C.COL_OBJECT_TAB, C.BPLOG_OBJTYPE_COL, C.BPLOG_OBJNAME_COL])
+
             # Filter and/or update the event names if needed/configured
-            df = super().transform(df)
-            return df
+            return super().transform(logs.content)
         
         except Exception as e:
-            self.log.error("transform() -> Unable to update the data " + str(e))
+            self.log.error("bppiPLRBluePrismRepo.transform() -> Unable to update the data " + str(e))
             return super().transform(df)
```

## utils/constants.py

```diff
@@ -95,28 +95,37 @@
 TRACE_FILENAME = "bppiapiwrapper.log"
 TRACE_MAXBYTES = 1000000
 
 # Dump file suffix
 TEMP_SQLDUMP = "-temp-sqlserver-dump.csv"
 
 # Blue Prism stuff
-BPLOG_STAGETYPE_COL = "stagetype"                   # Name of the stagetype column in the BP Repo
-BPLOG_STAGENAME_COL = "stagename"                   # Name of the stagename column in the BP Repo
+BPLOG_FIELD_LOGID = "logId"
+BPLOG_FIELD_SESSIONID = "SessionID"
+BPLOG_STARTDATETIME_COL = "resourceStartTime"       # Name of the Start Date & time column in the BP Repo
+BPLOG_FILTERDATE_COL = "LOG.startdatetime"
+BPLOG_RESOURCENAME_COL = "ResourceName"             # DW name
+BPLOG_STAGETYPE_COL = "stageType"                   # Name of the stagetype column in the BP Repo
+BPLOG_STAGENAME_COL = "stageName"                   # Name of the stagename column in the BP Repo
+BPLOG_RESULT_COL = 'result'                         # Execution result
+BPLOG_PAGENAME_COL = "pagename"                     # Only in PB repo
+BPLOG_ACTIONNAME_COL = "actionname"                 # Only in PB repo
+BPLOG_OBJTYPE_COL = "OBJECT_TYPE"                   # Only in PB repo
+BPLOG_OBJNAME_COL = 'OBJECT_NAME'                   # Only in PB repo
 BPLOG_PAGENAME_COL = "pagename"                     # Name of the pagename column in the BP Repo
 BPLOG_PROCESSNAME_COL = "processname"               # Name of the process name column in the BP Repo
-BPLOG_STARTDATETIME_COL = "startdatetime"           # Name of the Start Date & time column in the BP Repo
 BPLOG_ATTRIBUTE_COL = "attributexml"                # Name of the attributexml column in the BP Repo
 BPLOG_LOG_UNICODE = "BPASessionLog_Unicode"         # BP Log table name for unicode
 BPLOG_LOG_NONUNICODE = "BPASessionLog_NonUnicode"   # BP Log table name for non unicode
 BPLOG_INI4SQL = "bplogs.sql"                        # File which contains the BP SQL Query
 BP_STAGE_START = "Start"                            # Name of the BP Start stage
 BP_STAGE_END = "End"                                # Name of the BP End stage
 BP_MAINPAGE_DEFAULT = "Main Page"                   # Name of the BP Main Page (process)
 BP_DEFAULT_DELTAFILE = "bpdelta.tag"                # Default filename for the delta tag
 BP_DELTADATE_FMT = "%Y-%m-%d %H:%M:%S"              # Delta date format %Y-%m-%d %H:%M:%S
-COL_STAGE_ID = "STAGE_ID"
+COL_STAGE_ID = "stageId"
 COL_OBJECT_TAB = "OBJECT_TAB"
 
 # SQLite configuration SPecifics
 PARAM_SQ_ID = "id"                                  # When using SQLite config / ID of the config
 SQLITE_GETCONFIG = "SELECT * FROM VIEW_GET_FULLCONFIG_BLUEPRISM_REPO WHERE ID={}"
```

## Comparing `pyBPPIBridge-0.5.0.dist-info/LICENSE` & `pyBPPIBridge-0.5.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyBPPIBridge-0.5.0.dist-info/METADATA` & `pyBPPIBridge-0.5.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyBPPIBridge
-Version: 0.5.0
+Version: 0.5.1
 Summary: This solution builds a bridge between Blue Prism Process Intelligence (alias BPPI) and some external data sources (like files, databases via ODBC, SAP and Blue Prism).
 Author-email: Benoit Cayla <benoit@exypro.org>
 License: MIT License
         
         Copyright (c) 2023 Benoît Cayla
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
```

## Comparing `pyBPPIBridge-0.5.0.dist-info/RECORD` & `pyBPPIBridge-0.5.1.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -2,48 +2,49 @@
 bppibridgesq.py,sha256=N4TmQy-KktD62ECtQYvfOyTtr77zGqrQgurwUwOX_hQ,413
 bppibridge/__init__.py,sha256=JoPBB6E_bWNytR3vlupEdLWLpnUSYQMVlihzUb7wliI,569
 config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 config/appConfig.py,sha256=Zhn9sOOnQEnp9UseWDs43JQrdFH6o-0IKuUYtdV7v2c,4112
 config/cmdLineConfig.py,sha256=zxQPpQLTnz7rcPDMhPcVSQI2z7fFVAu-45xXzVZs6oI,3059
 pipelines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pipelines/pipeline.py,sha256=yjrpk4rW8z3stts4aU4ToM6UMIKHzyEWX6Wb_7feGzA,3437
-pipelines/pipelineFactory.py,sha256=x9z6fte7aUvHz5KTEf73Lyao3XTFehFVImw-Qagqp6I,4644
+pipelines/pipelineFactory.py,sha256=AXE24ZGoB4DzPJaWNeabQ5WDpZ4a3L-k4J66EWFXazg,4992
 pipelines/bppi/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pipelines/bppi/bppiPipeline.py,sha256=UCig65NnTW0MXkxQQ26Y5SMPUe0LWRNyMAeVaYISPlo,5058
 pipelines/bppi/uploadConfig.py,sha256=l1ffewmeUbnCjLg9xQqpCRQu_yQzGft-4usQRIk4h38,1384
 pipelines/bppi/project/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pipelines/bppi/project/bppiApiProjectWrapper.py,sha256=zgy-KDUXzNbxhtIJFmqh7dtHVQpMgDB1-K77NQeRBQc,969
 pipelines/bppi/project/bppiProject.py,sha256=sbDJfW5eL_A6ZSgvo7GCtMLV-ZMqjQ5DzSmz0jp97BA,523
 pipelines/bppi/repository/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pipelines/bppi/repository/bppiApiRepositoryWrapper.py,sha256=aGxEjkEvdlSyIu8ecKofZ8M2h_iW5qYmD08O9YbzQl8,8963
 pipelines/bppi/repository/bppiRepository.py,sha256=UiqysVk1gLYg6ejv5PiKdvDFEJsZifabmCCMGfZ5gcE,9166
 pipelines/bppi/repository/repConfig.py,sha256=LppAvoRBr2WBML_XHHRnkXHX1-PA_zklJOUX5mDDNXA,2195
+pipelines/buslogs/blueprismLogs.py,sha256=Mz-CuIs2fe56e6vea15xsEpmOlttVSDmIZOmnYwwr90,12364
 pipelines/project/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pipelines/readers/Reader.py,sha256=d9CnilYPZJ4pLiuAMHDCTHkmYzI7-IKQR0aFoPvpjKQ,679
 pipelines/readers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/readers/bpAPIReader.py,sha256=QRw4EnKyhOwnZIwHRspk1_RrRvDTOGc_QeOyB9WWOag,9613
+pipelines/readers/bpAPIReader.py,sha256=xQgEaVpUl-7Ni9TZgRIUOiX6Kz2OI4IIW2dp4XwXjT8,9665
 pipelines/readers/csvFileReader.py,sha256=MRJRPLNLo68fho2BE4D2za8PuMZTnfGTQODuB3YvYX0,1052
 pipelines/readers/excelFileReader.py,sha256=ucW11dW8tawoB7lQkCG2RvVSRXLpf2TYApo55pxPXM8,1105
 pipelines/readers/odbcReader.py,sha256=OSEwcuGt6w3IiV--NMuax-XBbVZ6X_y4GZkMKjZRw0U,1422
 pipelines/readers/sapRFCTableReader.py,sha256=UeFFuyVfSWKzPdo0d0px1OVJKKkS6o204Jrh7e4K5D8,3913
 pipelines/readers/xesFileReader.py,sha256=V7ux7OH4giFPW9Lfxdb2UrLhpyJWk28p3Hjimet_TOc,3989
 pipelines/readers/builders/SQLBuilder.py,sha256=8oV51y4wNOqzp-qqNEIcCoNaLYfI41nsZEQAmQfuCzA,2209
 pipelines/readers/builders/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/readers/builders/blueprismSQLBuilder.py,sha256=pe-WfydH8AqWvmluDnOv33b5uuV4mz2xq2-aZ289vPk,3676
+pipelines/readers/builders/blueprismSQLBuilder.py,sha256=PJIhXyJVcAtgHGKB1yxreyVofa5YOaeZWsyCn6J3UpQ,3648
 pipelines/repository/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pipelines/repository/bppiPLRBluePrismApi.py,sha256=JHSV8aLcNkbV1FEr3J2fCaz8qpeCVJgnevEeW0D15Y4,2164
-pipelines/repository/bppiPLRBluePrismRepo.py,sha256=LoDpjkgI6-k1KZcENo9zzVRf9EA6gApBOPgpni-b-Kw,12377
+pipelines/repository/bppiPLRBluePrismApi.py,sha256=sZoY2qrMeh-ugIvdpPC9Br0GV4jiiqZ_PE-Q0ZB54xk,3579
+pipelines/repository/bppiPLRBluePrismRepo.py,sha256=cuCnUvOCxG2kV8fT6DeK3aHUC3WME-notQCV63inNyI,6970
 pipelines/repository/bppiPLRCSVFile.py,sha256=Fr2-tCNHb7ukovoypNVKDwUFtVwymdCFB2xVKnLIsPE,1338
 pipelines/repository/bppiPLRChorusExtract.py,sha256=z2tQHkoIh8W3OqABnIymV3ncVTFpzDJRL0AgfYBa9Ls,788
 pipelines/repository/bppiPLRExcelFile.py,sha256=eu3EuLrWWJ0Z3dMiuNUNLL01l2pK-4xfwzxb24leJqE,1467
 pipelines/repository/bppiPLRODBC.py,sha256=Suvwk18PPjyTrKCx7Zovs83i7ZrYivNIK7d2ZIlA8ws,1888
 pipelines/repository/bppiPLRSAPRfcTable.py,sha256=VIi2U_AaaBlKl2es15jtjHC84QA9MbJxBfrw5U7-M9E,2590
 pipelines/repository/bppiPLRXESFile.py,sha256=y9plTprExkzk7BvXCVqXRtE70_GZ2YK2JPwDji14qhQ,1303
 utils/__init__.py,sha256=toHVlAEo46DsKhmOS5GRAJiEoNYR43sZkihAxwySMag,555
-utils/constants.py,sha256=lTghCBwhv4P5RIku51_c9q5Cf3Obhwm1WN96XmsSwsY,8001
+utils/constants.py,sha256=sfvfzJYVNovsxHFlYizMteZBukOTxhKN7T3YTTn9G_A,8529
 utils/log.py,sha256=-FL9vph0YXwuuvz3fNv0lISiYjigBE99k703XnS9qGY,2071
-pyBPPIBridge-0.5.0.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
-pyBPPIBridge-0.5.0.dist-info/METADATA,sha256=v6nOYtttTuensgt2Qonh_T3VSQCQpaGt-lOOCm5D71g,3868
-pyBPPIBridge-0.5.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-pyBPPIBridge-0.5.0.dist-info/entry_points.txt,sha256=pBbss7e8g9wopGxcuuy7UkA9WeAxvcVkP-TxlNaPYag,47
-pyBPPIBridge-0.5.0.dist-info/top_level.txt,sha256=MQN5L-3gd5Xf4J_8VYMNSt6s2gKGU6l4AmnrHyIWbWA,47
-pyBPPIBridge-0.5.0.dist-info/RECORD,,
+pyBPPIBridge-0.5.1.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
+pyBPPIBridge-0.5.1.dist-info/METADATA,sha256=xylqjP9R5Ik1A5-2vbHIW2UK8uByhl290FoZCPbWdo8,3868
+pyBPPIBridge-0.5.1.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+pyBPPIBridge-0.5.1.dist-info/entry_points.txt,sha256=pBbss7e8g9wopGxcuuy7UkA9WeAxvcVkP-TxlNaPYag,47
+pyBPPIBridge-0.5.1.dist-info/top_level.txt,sha256=MQN5L-3gd5Xf4J_8VYMNSt6s2gKGU6l4AmnrHyIWbWA,47
+pyBPPIBridge-0.5.1.dist-info/RECORD,,
```

